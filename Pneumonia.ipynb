{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNgSD+9B2ZDSh3w/1/drr0H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarsAmker/ExplainGAN/blob/master/Pneumonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmpTb0PZ50ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziaB1iqz9xgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pwd\n",
        "!mkdir ~/.kaggle\n",
        "#!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwimdKtue5rn",
        "colab_type": "text"
      },
      "source": [
        "### Upload the kaggle.json and the vgg16 and NAINsDepthwiseWeights files manually in the left hand pane"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YOj91hd0vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy the json file into the kaggle directory\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q85yyv5K7OiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pneumonia data (first line makes kaggle API key unreadable)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "# the zip file is listed in the left hand pane after the download. We need to unzip and create paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsvFZPXofXpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip chest-xray-pneumonia.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXUkM73Hh5Wz",
        "colab_type": "text"
      },
      "source": [
        "# NAIN's kaggle kernel\n",
        "\n",
        "Now try to rebuild the top kernel from kaggle (https://www.kaggle.com/aakashnain/beating-everything-with-depthwise-convolution). This is the classifier I would like to explain with ExplainGAN.\n",
        "\n",
        "Modifications compared to the original code of the kernel are mentioned when they appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-lNB19g0PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import shutil\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import imgaug.augmenters as iaa\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCWnHofyg9om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My directory is called \"content\" instead of \"input\" as in the original code\n",
        "os.listdir(\"../content/chest_xray/chest_xray\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvEF_xzy7mL1",
        "colab_type": "text"
      },
      "source": [
        "Reproducibility is a great concern when doing deep learning. There was a good discussion on KaggleNoobs slack regarding this. We will set a number of things in order to make sure that the results are almost reproducible(if not fully)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VoSoOQjf5ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import tensorflow as tf\n",
        "\n",
        "# Set the seed for hash based operations in python\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "# Set the numpy seed\n",
        "np.random.seed(111)\n",
        "\n",
        "# Disable multi-threading in tensorflow ops\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "# Set the random seed in tensorflow at graph level\n",
        "tf.set_random_seed(111)\n",
        "\n",
        "# Define a tensorflow session with above session configs\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "\n",
        "# Set the session in keras\n",
        "K.set_session(sess)\n",
        "\n",
        "# Make the augmentation sequence deterministic\n",
        "aug.seed(111)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi2uV41X7qVs",
        "colab_type": "text"
      },
      "source": [
        "The dataset is divided into three sets: 1) train set 2) validation set and 3) test set. Let's grab the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c57VWXTohsom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define path to the data directory: \"content\" instead of \"input/chest-xray-pneumonia\" as in the original code\n",
        "data_dir = Path('../content/chest_xray/chest_xray')\n",
        "\n",
        "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
        "train_dir = data_dir / 'train'\n",
        "\n",
        "# Path to validation directory\n",
        "val_dir = data_dir / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_dir = data_dir / 'test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hXJKM3M7uLr",
        "colab_type": "text"
      },
      "source": [
        "We will first go through the training dataset. We will do some analysis on that, look at some of the samples, check the number of samples for each class, etc. Lets' do it.\n",
        "Each of the above directory contains two sub-directories:\n",
        "\n",
        "NORMAL: These are the samples that describe the normal (no pneumonia) case.\n",
        "\n",
        "PNEUMONIA: This directory contains those samples that are the pneumonia cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNl_MYGri6Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the path to the normal and pneumonia sub-directories\n",
        "normal_cases_dir = train_dir / 'NORMAL'\n",
        "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
        "\n",
        "# Get the list of all the images\n",
        "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "\n",
        "# An empty list. We will insert the data into this list in (img_path, label) format\n",
        "train_data = []\n",
        "\n",
        "# Go through all the normal cases. The label for these cases will be 0\n",
        "for img in normal_cases:\n",
        "    train_data.append((img,0))\n",
        "\n",
        "# Go through all the pneumonia cases. The label for these cases will be 1\n",
        "for img in pneumonia_cases:\n",
        "    train_data.append((img, 1))\n",
        "\n",
        "# Get a pandas dataframe from the data we have in our list \n",
        "train_data = pd.DataFrame(train_data, columns=['image', 'label'],index=None)\n",
        "\n",
        "# Shuffle the data \n",
        "train_data = train_data.sample(frac=1.).reset_index(drop=True)\n",
        "\n",
        "# How the dataframe looks like?\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyjEp7cX8KtA",
        "colab_type": "text"
      },
      "source": [
        "###Preparing validation data\n",
        "We will be defining a generator for the training dataset later in the notebook but as the validation data is small, so I can read the images and can load the data without the need of a generator. This is exactly what the code block given below is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7umJVpQgpYc9",
        "colab_type": "text"
      },
      "source": [
        "# Focal loss: Different from here on. Starting with code box 11 of the kaggle kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOAlztNV-Oem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Why do we make artificial RGB pictures? Because the weights from imageNet that we import are for RGB pictures\n",
        "# Get the path to the sub-directories\n",
        "normal_cases_dir = val_dir / 'NORMAL'\n",
        "pneumonia_cases_dir = val_dir / 'PNEUMONIA'\n",
        "\n",
        "# Get the list of all the images\n",
        "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "\n",
        "# List that are going to contain validation images data and the corresponding labels\n",
        "valid_data = []\n",
        "valid_labels = []\n",
        "\n",
        "# Some images are in grayscale while majority of them contains 3 channels. \n",
        "# So, if the image is grayscale, we will convert into a image with 3 channels.\n",
        "# We will normalize the pixel values and resizing all the images to 224x224 \n",
        "\n",
        "# Normal cases\n",
        "for img in normal_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img]) # add a third dimension for RGB channels and fill it with 3 copies of the original\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(0, num_classes=2)\n",
        "    valid_data.append(img)\n",
        "    valid_labels.append(label)\n",
        "                      \n",
        "# Pneumonia cases        \n",
        "for img in pneumonia_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(1, num_classes=2)\n",
        "    valid_data.append(img)\n",
        "    valid_labels.append(label)\n",
        "    \n",
        "# Convert the list into numpy arrays\n",
        "valid_data = np.array(valid_data)\n",
        "valid_labels = np.array(valid_labels)\n",
        "\n",
        "print(\"Total number of validation examples: \", valid_data.shape)\n",
        "print(\"Total number of labels:\", valid_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr_2t1c9-cmG",
        "colab_type": "text"
      },
      "source": [
        "###Augmentation (stop ExplainGAN preprocessing before this?)\n",
        "Data augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. But augmentation can be much more helpful where the dataset is imbalanced. You can generate different samples of undersampled class in order to try to balance the overall distribution.\n",
        "I like imgaug a lot. It comes with a very clean api and you can do hell of augmentations with it. It's worth exploring!! In the next code block, I will define a augmentation sequence. You will notice Oneof and it does exactly that. At each iteration, it will take one augmentation technique out of the three and will apply that on the samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3vBbZ_c-nMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Augmentation sequence \n",
        "seq = iaa.OneOf([\n",
        "    iaa.Fliplr(), # horizontal flips\n",
        "    iaa.Affine(rotate=20), # roatation\n",
        "    iaa.Multiply((1.2, 1.5))]) #random brightness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXXQBH1C-q_F",
        "colab_type": "text"
      },
      "source": [
        "###Training data generator\n",
        "\n",
        "Here I will define a very simple data generator. You can do more than this if you want but I think at this point, this is more than enough I need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EawVbLY-xz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(data, batch_size):\n",
        "    # Get total number of samples in the data\n",
        "    n = len(data)\n",
        "    steps = n//batch_size\n",
        "    \n",
        "    # Define two numpy arrays for containing batch data and labels\n",
        "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((batch_size,2), dtype=np.float32)\n",
        "\n",
        "    # Get a numpy array of all the indices of the input data\n",
        "    indices = np.arange(n)\n",
        "    \n",
        "    # Initialize a counter\n",
        "    i =0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        # Get the next batch \n",
        "        count = 0\n",
        "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            img_name = data.iloc[idx]['image']\n",
        "            label = data.iloc[idx]['label']\n",
        "            \n",
        "            # one hot encoding\n",
        "            encoded_label = to_categorical(label, num_classes=2)\n",
        "            # read the image and resize\n",
        "            img = cv2.imread(str(img_name))\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            \n",
        "            # check if it's grayscale\n",
        "            if img.shape[2]==1:\n",
        "                img = np.dstack([img, img, img])\n",
        "            \n",
        "            # cv2 reads in BGR mode by default\n",
        "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            # normalize the image pixels\n",
        "            orig_img = img.astype(np.float32)/255.\n",
        "            \n",
        "            batch_data[count] = orig_img\n",
        "            batch_labels[count] = encoded_label\n",
        "            \n",
        "            # generating more samples of the undersampled class\n",
        "            if label==0 and count < batch_size-2:\n",
        "                aug_img1 = seq.augment_image(img)\n",
        "                aug_img2 = seq.augment_image(img)\n",
        "                aug_img1 = cv2.cvtColor(aug_img1, cv2.COLOR_BGR2RGB)\n",
        "                aug_img2 = cv2.cvtColor(aug_img2, cv2.COLOR_BGR2RGB)\n",
        "                aug_img1 = aug_img1.astype(np.float32)/255.\n",
        "                aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "\n",
        "                batch_data[count+1] = aug_img1\n",
        "                batch_labels[count+1] = encoded_label\n",
        "                batch_data[count+2] = aug_img2\n",
        "                batch_labels[count+2] = encoded_label\n",
        "                count +=2\n",
        "            \n",
        "            else:\n",
        "                count+=1\n",
        "            \n",
        "            if count==batch_size-1:\n",
        "                break\n",
        "            \n",
        "        i+=1\n",
        "        yield batch_data, batch_labels\n",
        "            \n",
        "        if i>=steps:\n",
        "            i=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLX5Kddl-zhj",
        "colab_type": "text"
      },
      "source": [
        "###Model\n",
        "This is the best part. If you look at other kernels on this dataset, everyone is busy doing transfer learning and fine-tuning. You should transfer learn but wisely. We will be doing partial transfer learning and rest of the model will be trained from scratch. I will explain this in detail but before that, I would love to share one of the best practices when it comes to building deep learning models from scratch on limited data.\n",
        "\n",
        "1 Choose a simple architecture.\n",
        "\n",
        "2 Initialize the first few layers from a network that is pretrained on imagenet. This is because first few layers capture general details like color blobs, patches, edges, etc. Instead of randomly initialized weights for these layers, it would be much better if you fine tune them.\n",
        "\n",
        "3 Choose layers that introduce a lesser number of parameters. For example, Depthwise SeparableConv is a good replacement for Conv layer. It introduces lesser number of parameters as compared to normal convolution and as different filters are applied to each channel, it captures more information. Xception a powerful network, is built on top of such layers only. You can read about Xception and Depthwise Separable Convolutions in this paper.\n",
        "\n",
        "4 Use batch norm with convolutions. As the network becomes deeper, batch norm start to play an important role.\n",
        "\n",
        "5 Add dense layers with reasonable amount of neurons. Train with a higher learning rate and experiment with the number of neurons in the dense layers. Do it for the depth of your network too. \n",
        "\n",
        "6 Once you know a good depth, start training your network with a lower learning rate along with decay. \n",
        "\n",
        "This is all that I have done in the next code block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j76t5VFIjUwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# changed the format of this method due to compatibility with tensorflow 2\n",
        "# The original version from NAIN worked with \"x=...(x)\" steps, my theory is that this x is the placeholder that lead to errors\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv1_1', input_shape=[224, 224, 3]))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv1_2', input_shape=[224, 224, 3]))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool'))\n",
        "\n",
        "  model.add(layers.SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1'))\n",
        "  model.add(layers.SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2'))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool2'))\n",
        "\n",
        "  model.add(layers.SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1'))\n",
        "  model.add(layers.BatchNormalization(name='bn1'))\n",
        "  model.add(layers.SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2'))\n",
        "  model.add(layers.BatchNormalization(name='bn2'))\n",
        "  model.add(layers.SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3'))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool3'))\n",
        "\n",
        "  model.add(layers.SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1'))\n",
        "  model.add(layers.BatchNormalization(name='bn3'))\n",
        "  model.add(layers.SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2'))\n",
        "  model.add(layers.BatchNormalization(name='bn4'))\n",
        "  model.add(layers.SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3'))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool4'))\n",
        "\n",
        "  model.add(layers.Flatten(name='flatten'))\n",
        "  model.add(layers.Dense(1024, activation='relu', name='fc1'))\n",
        "  model.add(layers.Dropout(0.7, name='dropout1'))\n",
        "  model.add(layers.Dense(512, activation='relu', name='fc2'))\n",
        "  model.add(layers.Dropout(0.5, name='dropout2'))\n",
        "  model.add(layers.Dense(2, activation='softmax', name='fc3'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4K3k3vn_Roz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =  build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed9LPd2uOV1d",
        "colab_type": "text"
      },
      "source": [
        "### Upload the vgg16 file manually in the left hand pane\n",
        "We will initialize the weights of first two convolutions with imagenet weights. They are contained in the vgg16 file listed on NAIN's kernel under data. Unzip it after uploading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0GlLiYta_OQ",
        "colab_type": "code",
        "outputId": "258cd9a8-78be-4278-b7f9-aca2dff2f9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!unzip vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5.zip"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5.zip\n",
            "  inflating: vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNxl96CGOW0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open the VGG16 weight file.\n",
        "f = h5py.File('../content/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n",
        "\n",
        "# Select the layers for which you want to set weight.\n",
        "\n",
        "w,b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1']['block1_conv1_b_1:0']\n",
        "model.layers[1].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2']['block1_conv2_b_1:0']\n",
        "model.layers[2].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv1']['block2_conv1_W_1:0'], f['block2_conv1']['block2_conv1_b_1:0']\n",
        "model.layers[4].set_weights = [w,b]\n",
        "\n",
        "w,b = f['block2_conv2']['block2_conv2_W_1:0'], f['block2_conv2']['block2_conv2_b_1:0']\n",
        "model.layers[5].set_weights = [w,b]\n",
        "\n",
        "f.close()\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI5o5UGccsam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
        "es = EarlyStopping(patience=5)\n",
        "chkpt = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE_TWWNyc8dd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7d1668f-9398-40ec-f92a-0e786987abfa"
      },
      "source": [
        "batch_size = 16\n",
        "nb_epochs = 20\n",
        "\n",
        "# Get a train data generator\n",
        "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "\n",
        "# Define the number of training steps\n",
        "nb_train_steps = train_data.shape[0]//batch_size\n",
        "\n",
        "print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(valid_data)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training and validation steps: 326 and 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DClX1GoU6oQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "5d8306c3-6c23-44e0-eb81-e4d623d7625b"
      },
      "source": [
        "valid_labels"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh7Q2cqfdH4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "e19946e8-2f8a-4547-b357-7298276558f2"
      },
      "source": [
        "# Fit the model. I deleted sth about early stopping and checkpoints because of an error. But that should not be the problem...\n",
        "# Problem: The model predicts \"healthy\" for every image now. The training doesn't really do anything.\n",
        "# Maybe we need to change the loss function from binary crossentropy?\n",
        "# Plan: Change the format of the labels s.t. categorical_sparse_crossentropy can be used. That's what worked for MNIST.\n",
        "history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n",
        "                              validation_data=(valid_data, valid_labels),\n",
        "                              class_weight={0:1.0, 1:0.4})"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-f923da20d591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n\u001b[1;32m      2\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                               class_weight={0:1.0, 1:0.4})\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    782\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must have the same first dimension, got logits shape [16,2] and labels shape [32]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-57-f923da20d591>:3) ]] [Op:__inference_train_function_30316]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Pynl6EfAI8",
        "colab_type": "code",
        "outputId": "bb4bf227-0fa7-44d2-ebda-efd097616a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Preparing test data\n",
        "normal_cases_dir = test_dir / 'NORMAL'\n",
        "pneumonia_cases_dir = test_dir / 'PNEUMONIA'\n",
        "\n",
        "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "\n",
        "test_data = []\n",
        "test_labels = []\n",
        "\n",
        "for img in normal_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(0, num_classes=2)\n",
        "    test_data.append(img)\n",
        "    test_labels.append(label)\n",
        "                      \n",
        "for img in pneumonia_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(1, num_classes=2)\n",
        "    test_data.append(img)\n",
        "    test_labels.append(label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(\"Total number of test examples: \", test_data.shape)\n",
        "print(\"Total number of labels:\", test_labels.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of test examples:  (624, 224, 224, 3)\n",
            "Total number of labels: (624, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waOSOfxifEIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7d7464fb-84ee-4ede-cc65-5d2a97d1ebd8"
      },
      "source": [
        "# Evaluation on test dataset\n",
        "test_loss, test_score = model.evaluate(test_data, test_labels, batch_size=16)\n",
        "print(\"Loss on test set: \", test_loss)\n",
        "print(\"Accuracy on test set: \", test_score)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 2s 47ms/step - loss: 9.5857 - accuracy: 0.3750\n",
            "Loss on test set:  9.585684776306152\n",
            "Accuracy on test set:  0.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thif7JOcfG0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5ed7628f-765f-4fd9-87ee-369bded2785a"
      },
      "source": [
        "# Get predictions\n",
        "preds = model.predict(test_data, batch_size=16)\n",
        "preds = np.argmax(preds, axis=-1)\n",
        "\n",
        "# Original labels\n",
        "orig_test_labels = np.argmax(test_labels, axis=-1)\n",
        "\n",
        "print(orig_test_labels.shape)\n",
        "print(preds.shape)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(624,)\n",
            "(624,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlLiJmXkfLv_",
        "colab_type": "text"
      },
      "source": [
        "When a particular problem includes an imbalanced dataset, then accuracy isn't a good metric to look for. For example, if your dataset contains 95 negatives and 5 positives, having a model with 95% accuracy doesn't make sense at all. The classifier might label every example as negative and still achieve 95% accuracy. Hence, we need to look for alternative metrics. Precision and Recall are really good metrics for such kind of problems.\n",
        "\n",
        "We will get the confusion matrix from our predictions and see what is the recall and precision of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0BKRPbUfO6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "859813a6-a27e-442c-9cc4-9bbb37f5754a"
      },
      "source": [
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(orig_test_labels, preds)\n",
        "plt.figure()\n",
        "#plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, alpha=0.7,cmap=plt.cm.Blues)\n",
        "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
        "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAHlCAYAAADr6sZuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0GR4luFfU2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate Precision and Recall\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "\n",
        "print(\"Recall of the model is {:.2f}\".format(recall))\n",
        "print(\"Precision of the model is {:.2f}\".format(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GN9yS9Vrxeg",
        "colab_type": "text"
      },
      "source": [
        "### My Results\n",
        "My confusion matrix after 20 epochs of training: 150,84,10,380 -> Slightly lower recall, better precision and better accuracy than NAIN's best model. In other runs, it was the other way around and my model was leaning more towards predicting pneumonia.\n",
        "NAIN has 134,100,8,382 (accuracy 0.827, recall 0.98, precision 0.79)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV1eZniX3TSx",
        "colab_type": "text"
      },
      "source": [
        "### Saving and reloading the weights\n",
        "In the future manually upload the saved weights and only do the reloading instead of training to save time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqCNPsDqyqqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('NAINsDepthwise.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4WdM8INzztc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('NAINsDepthwiseWeights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIPRewwZ1JiO",
        "colab_type": "code",
        "outputId": "4fbbcc57-12b0-4e62-a022-007609212cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "new_model = build_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpeXfbxS2vNg",
        "colab_type": "code",
        "outputId": "921e9969-b46c-402a-e7fa-f3802fe5c254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "opt = Adam(lr=0.0001, decay=1e-5)\n",
        "new_model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr5B2ja22OoN",
        "colab_type": "code",
        "outputId": "0c3697d3-2bb0-4869-b052-e8565aeea9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "new_model.load_weights('NAINsDepthwiseWeights')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-eff27b8e5be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NAINsDepthwiseWeights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (truncated file: eof = 375390208, sblock->base_addr = 0, stored_eof = 416857616)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8lDd0g11iXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_test_loss, new_test_score = new_model.evaluate(test_data, test_labels, batch_size=16)\n",
        "print(\"Loss on test set: \", new_test_loss)\n",
        "print(\"Accuracy on test set: \", new_test_score)\n",
        "new_preds = new_model.predict(test_data, batch_size=16)\n",
        "new_preds = np.argmax(new_preds, axis=-1)\n",
        "\n",
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(orig_test_labels, new_preds)\n",
        "plt.figure()\n",
        "#plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, alpha=0.7,cmap=plt.cm.Blues)\n",
        "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
        "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}