{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ExplainGAN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarsAmker/ExplainGAN/blob/master/ExplainGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_jQ1tEQCxwRx"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "V_sgB_5dx1f1",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "# ExplainGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0TD5ZrvEMbhZ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/dcgan\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/dcgan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ITZuApL56Mny"
      },
      "source": [
        "Try to build code for ExplainGAN starting with the DCGAN tutorial from tensorflow. \n",
        "\n",
        "Original text: This tutorial demonstrates how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5oue0oqCkZZ",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WZKbyU2-AiY-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError: # If TPU not found\n",
        "  tpu = None\n",
        "\n",
        "# To generate GIFs. Let's see, maybe this can still be useful here?\n",
        "!pip install imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YfIk2es3hJEd",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load and prepare the dataset\n",
        "\n",
        "Let's try the **MNIST** dataset here too. It was also one of the examples from Silberman's paper. We need the images showing **4s and 9s only**, because ExplainGAN explains binary classifier. (4,9) was one of the pairs of digits used by Silberman too (the others were (3,8) and (5,6)).\n",
        "Filter these out for the training part (60000) as well as for the test part (10000 images) of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4fYMGxGhrna",
        "colab": {}
      },
      "source": [
        "(all_train_images, all_train_labels), (all_test_images, all_test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Throw away everything except the images showing 4s or 9s\n",
        "# First just check how many images we should get for each of the digits (they don't appear in the dataset with the same frequency):\n",
        "count4 = (all_train_labels==4).sum()\n",
        "count9 = (all_train_labels==9).sum()\n",
        "#print(count4, count9, 'Number of images remaining:', count4+count9)\n",
        "\n",
        "# change labels to 0 and 1 in order to have a softmax with 2 labels (not 10 to reach 9) at the end of our classifier\n",
        "train_images = []\n",
        "train_labels = []\n",
        "for i in range(len(all_train_labels)):\n",
        "  if all_train_labels[i] == 4:\n",
        "    train_images.append(all_train_images[i])\n",
        "    train_labels.append(0)    \n",
        "  if all_train_labels[i] == 9:\n",
        "    train_images.append(all_train_images[i])\n",
        "    train_labels.append(1)\n",
        "\n",
        "# Change formats from list back to array. This is necessary for the reshaping in the next step    \n",
        "train_images = np.asarray(train_images)\n",
        "train_labels = np.asarray(train_labels)\n",
        "# Check works out, we have as many images and labels as we should have and the images match their labels (tested the first few ones with plt.imshow): \n",
        "#print(train_labels)\n",
        "#print(len(train_images), len(train_labels))\n",
        "#plt.imshow(train_images[0], cmap='gray')\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "# Normalize the images to [-1, 1]\n",
        "train_images = (train_images-127.5) / 127.5 \n",
        "\n",
        "#train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQkaE7Gf1jpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do the same filtering and reshaping for the test set:\n",
        "count4 = (all_test_labels==4).sum()\n",
        "count9 = (all_test_labels==9).sum()\n",
        "#print(count4, count9, 'Number of images remaining:', count4+count9)\n",
        "\n",
        "# change labels to 0 and 1 in order to have a softmax with 2 labels (not 10 to reach 9) at the end of our classifier\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for i in range(len(all_test_labels)):\n",
        "  if all_test_labels[i] == 4:\n",
        "    test_images.append(all_test_images[i])\n",
        "    test_labels.append(0)    \n",
        "  if all_test_labels[i] == 9:\n",
        "    test_images.append(all_test_images[i])\n",
        "    test_labels.append(1)\n",
        "    \n",
        "# Change formats from list back to array. This is necessary for the reshaping in the next step    \n",
        "test_images = np.asarray(test_images)\n",
        "test_labels = np.asarray(test_labels)\n",
        "# Check works out, we have as many images and labels as we should have and the images match their labels (tested the first few ones with plt.imshow): \n",
        "#print(test_labels)\n",
        "#print(len(test_images), len(test_labels))\n",
        "#plt.imshow(test_images[0], cmap='gray')\n",
        "\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
        "# Normalize the images to [-1, 1]\n",
        "test_images = (test_images - 127.5) / 127.5\n",
        "\n",
        "#test_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Our classifier\n",
        "\n",
        "First we need a pre-trained **binary** classifier. Modify the one from the tensorflow tutorial \"Basic Image Classification\" on Fashion-MNIST classification.\n",
        "\n",
        "This classifier is the AI that we aim to explain with ExplainGAN. It should not interact a lot with the ExplainGAN part (except for being used by it at the end of the process) and thus be fairly interchangeable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v8FZCJQ007I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretty much copied from tf tutorial \"Basic image classification\". Changed the input_shape in Flatten and only 2 nodes in Softmax\n",
        "classifier = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28,1)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "classifier.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "classifier.fit(train_images, train_labels, epochs=10)\n",
        "\n",
        "test_loss, test_acc = classifier.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIHkTKq23jvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the classifier\n",
        "#predicted_classes = classifier.predict(test_images[0:1,:,:,:])\n",
        "# The prediction is (as before, 0 means 4 and 1 means 9)\n",
        "#predicted_classes\n",
        "#np.argmax(predicted_classes, axis=1) # axis=1 needed to look at every couple of values, not the whole tensor at once"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyPs77Qixr2h",
        "colab_type": "text"
      },
      "source": [
        "## The actual ExplainGAN part\n",
        "\n",
        "On top of that, there are two encoders (not present in DCGAN), 3 generators (Sharing the first few layers) that produce three images (reconstruction, transformation and mask) and two discriminators used for training this generator. My interpretation of Silberman's very sparse paragraph about the ExplainGAN model architecture is that the encoders and discriminators are similar to the DCGAN discriminator and the generator is similar to DCGAN's generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TivYEDyhEUsL",
        "colab_type": "text"
      },
      "source": [
        "### The encoders\n",
        "\n",
        "There are two encoders, one for each predicted class. They take an image and produce a compressed, encoded so-called latent variable z that the generator uses as input. In the tf tutorial for DCGAN on MNIST, the generator's input was an array of 100 standard normal r.v.s. Let's go for a latent variable of size 128 here (in Silberman's EcplainGAN paper, there is no information about the dimension of the encoded array). I also looked at the Variational Auto Encoder tf tutorial, but it is kind of complicated, so I went for the source below instead (which is also using MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-IG8hUxFQb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copied from https://blog.keras.io/building-autoencoders-in-keras.html, added flattening at the end\n",
        "# This is the encoder part of a conv. autoencoder applied to MNIST. Notice the MaxPooling. Maybe change later on\n",
        "def make_encoder_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(16, (3, 3), activation='sigmoid', padding='same', use_bias=True,\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Conv2D(8, (3, 3), activation='sigmoid', padding='same', use_bias=True))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Conv2D(8, (3, 3), activation='sigmoid', padding='same', use_bias=True))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    return model\n",
        "# At this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "# The convolutions here don't decrease the 2D dimension because they have default (1,1) strides. The MaxPooling does"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q65nZbN1Rn3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder4 = make_encoder_model()\n",
        "encoder9 = make_encoder_model()\n",
        "#encoder4.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### The generator and the mask function\n",
        "\n",
        "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from the latent variable z produced from one of the encoders. Start with a `Dense` layer that takes z as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh.\n",
        "\n",
        "Only change made compared to the DCGAN architecture: Input size is 128 now for compatibility with the encoders. As an alternaive to the DCGAN generator, we could use the decoder from the autoencoder source (used for the encoders above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6bpTcDqoLWjY",
        "colab": {}
      },
      "source": [
        "# This generator is based on the DCGAN generator. But now, we need 3 outputs and not just one!\n",
        "# Therefore tear it apart in the middle. We will use the second part three times to get recon and trafo and mask \n",
        "def make_generator_model_start():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*128, use_bias=False, input_shape=(128,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Create images of size 7*7 with 128 channels, all connected to the 128 nodes of the encoded original image\n",
        "    # In the next 2 lines, the tensor is actually reshaped into that channel form\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 128)))\n",
        "    assert model.output_shape == (None, 7, 7, 128) # Note: None is the batch size\n",
        "\n",
        "    # now move 5*5*128(channels) filters over the 7*7*128(channels) images in 1,1 strides. Do this for each of the 64 output channels.\n",
        "    # 64 is the number of different filters we apply. Because of the big number of input channels, each filter is already huge\n",
        "    # The number of parameters here is 5*5*128 (filter weights) *64 (number of filters)\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    return model\n",
        "\n",
        "generator_start = make_generator_model_start()\n",
        "#generator_start.summary()\n",
        "# Batch normalization has 4 parameters per channel. Two of them are trainable (gamma and beta mentioned in Szegedy)\n",
        "# The other 2 are not trainable (maybe epsilon and momentum?). I found this out by experimentation with a toy model."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0ji35nkGLT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now the second part. Apply this three times to get reconstruction and transformation and mask\n",
        "# Because of the splitting, I also needed to add the input shape in the first layer of this second part.\n",
        "def make_generator_model_end():\n",
        "    \n",
        "    # By taking strides of 2, the size of the image gets doubled in length and width.\n",
        "    # This is the case, because we do a backwards convolution. If we get a 7*7 image by taking (2,2)-strides, we must have started with 14*14\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(7,7,64)))\n",
        "    assert model.output_shape == (None, 14, 14, 32)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model\n",
        "  \n",
        "reconstructor = make_generator_model_end()\n",
        "transformator = make_generator_model_end()\n",
        "mask = make_generator_model_end()\n",
        "#reconstructor.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Use the (as yet untrained) generators to create some example images for tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gl7jcC7TdPTG",
        "colab": {}
      },
      "source": [
        "# Generate example images for some checks\n",
        "noise = tf.random.normal([1, 128]) # make this a 128 too to match the size of the latent variable that should actually be at this place\n",
        "trafo_image = transformator(generator_start(noise, training=False)) # instead of transformator, we can also use reconstructor here\n",
        "recon_image = reconstructor(generator_start(noise, training=False))\n",
        "mask_example = mask(generator_start(noise, training=False))\n",
        "#tf.add(tf.math.multiply(1-mask_example,trafo_image), tf.math.multiply((mask_example),recon_image))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(trafo_image[0, :, :, 0], cmap='gray')\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(recon_image[0, :, :, 0], cmap='gray')\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(mask_example[0,:,:,0], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YocrqcaATlOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only checking\n",
        "#tf.reshape(recon_image[:,0:2,0:2,:],[-1])\n",
        "#mask_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEladWfPIU0W",
        "colab_type": "text"
      },
      "source": [
        "### Create composite images\n",
        "\n",
        "Now that we have a reconstruction, transformation and the mask which will be trained to show the differences, we still need to combine these results with the original input images to get the composite images - our final product that we feed into the discriminators (along with the recon and trafo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSTNdIXwJDOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will need to call this function twice to create a composite image of each class\n",
        "# orig_image and created_image are always from the two different classes. \n",
        "# tf.math.multiply performs an element-wise multiplication\n",
        "def create_composite(orig_image, created_image, mask):\n",
        "  composite = tf.add(tf.math.multiply(1-mask,orig_image), tf.math.multiply((mask),created_image))\n",
        "  return composite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The discriminators\n",
        "\n",
        "The discriminator is a CNN-based image classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dw2tPLmk2pEP",
        "colab": {}
      },
      "source": [
        "# Mostly copied from the tf DCGAN tutorial. We will have two of them, one for each class\n",
        "# Modification: Put sigmoid at the end (nothing there before) to get values between 0 and 1\n",
        "# In the Silberman paper they say that these 2 discriminators share their last few layers.\n",
        "# Maybe split it up like I did with the generator later on\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', use_bias=True,\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    # number of params for Conv2D: 64(output channels = number of filters) * 26(filter size 5*5*1 + 1 for a bias node)\n",
        "    # In contrast to the generator model, we use bias nodes here. Maybe that would have been useless in the generator because of the batch normalizations\n",
        "    # They normalize the input anyway, so any bias would get cancelled again\n",
        "    # We also use dropout here!\n",
        "    \n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n",
        "    #number of params for this Conv2D: 128(output channels) * 1601(filter size 5*5*64 + 1 for a bias node)\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1, use_bias=True, activation=tf.nn.sigmoid)) \n",
        "    # sigmoid to get value between 0 and 1 (Goodfellow2016 did this in their DCGAN too, so it should be a good idea here)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gDkA05NE6QMs",
        "colab": {}
      },
      "source": [
        "discriminator4 = make_discriminator_model()\n",
        "discriminator9 = make_discriminator_model()\n",
        "#decision = discriminator9(trafo_image)\n",
        "#print (decision)\n",
        "#discriminator0.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss and optimizers\n",
        "\n",
        "Define all loss functions and optimizers needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOksh9BoPcFY",
        "colab_type": "text"
      },
      "source": [
        "### GAN loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nza0TjGfG-7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the GAN loss: The first two lines of formula (8) from Silberman are about original images x predicted as class j\n",
        "# The other 2 lines are about images predicted as class 1-j -> Sort differently here, put all same predictions together\n",
        "\n",
        "# real_output is the array of discriminator predictions on the real images\n",
        "# recon_output is the array of discriminator predictions on the recon images\n",
        "def loss_gan(real_output, recon_output, trafo_output, comp_output):\n",
        "  real_loss = tf.math.log(real_output)\n",
        "  recon_loss = tf.math.log(1-recon_output)\n",
        "  trafo_loss = tf.math.log(1-trafo_output) # use the other discriminator for these two\n",
        "  comp_loss = tf.math.log(1-comp_output)\n",
        "  return real_loss + recon_loss + trafo_loss + comp_loss\n",
        "\n",
        "# Idea for the future: Use one- (or 2-) sided label smoothing to keep gradients finite (--> Goodfellow2016)\n",
        "def loss_gan_1side_smooth(real_output, recon_output, trafo_output, comp_output):\n",
        "  real_loss = tf.math.log(real_output)*0.9 + tf.math.log(1-real_output)*0.1\n",
        "  recon_loss = tf.math.log(1-recon_output)\n",
        "  trafo_loss = tf.math.log(1-trafo_output) # use the other discriminator for these two\n",
        "  comp_loss = tf.math.log(1-comp_output)\n",
        "  return real_loss + recon_loss + trafo_loss + comp_loss\n",
        "\n",
        "# define the parts for checks why the GAN loss gets positive in the training loop\n",
        "# the error was found, these functions are not used anymore now. But useful to find out which losses explode\n",
        "def loss_gan_real(output):\n",
        "  return tf.math.log(output)\n",
        "def loss_gan_recon(output):\n",
        "  return tf.math.log(1-output)\n",
        "def loss_gan_trafo(output):\n",
        "  return tf.math.log(1-output)\n",
        "def loss_gan_comp(output):\n",
        "  return tf.math.log(1-output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkpcWwWtPf6d",
        "colab_type": "text"
      },
      "source": [
        "### Classifier loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SapWkWE7c6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# approach with two functions to avoid if conditions:\n",
        "loss_c_boundary = 20 # introduce an upper border for this loss. Reason: At the beginning of training, the mask is close to 0\n",
        "# This makes the composites resemble the originals, s.t. the classifier still predicts them as orig. class with super high probability\n",
        "# This made the loss go up all the way to actual infinity\n",
        "# Is this upper border idea okay? It seems to work at least, now this loss does not make problems\n",
        "def loss_classifier4(classifier_result): # used for predicted class 4 images, put comp.s looking like 9s in here\n",
        "  return np.minimum(-tf.math.log(classifier_result),loss_c_boundary)\n",
        "\n",
        "def loss_classifier9(classifier_result): # used for predicted class 9 images\n",
        "  return np.minimum(-tf.math.log(1-classifier_result),loss_c_boundary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KWdSf66Vxz8",
        "colab_type": "text"
      },
      "source": [
        "### Reconstruction loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAvjrPvXRcxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inputs of loss_recon are two images, the original one and its reconstruction \n",
        "def loss_recon(x, reconstruction):\n",
        "  difference = tf.subtract(x,reconstruction)\n",
        "  # reshape 28*28 to one vector in order to apply the l2 norm to it\n",
        "  # The -1 in the first dimension make sure that the number of images stays the same, images shall not be combined\n",
        "  difference = tf.reshape(difference, [-1,784,1])\n",
        "  return tf.math.square(tf.norm(difference, ord=2, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwx8cfzajX7b",
        "colab_type": "text"
      },
      "source": [
        "### Prior losses\n",
        "\n",
        "These losses are used to make sure we get interpretable and visible changes instead of small perturbations of a lot of pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf0bB5FltFRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_const(x, transformation, mask):\n",
        "  x_unmasked = tf.math.multiply(x, 1-mask)\n",
        "  transformation_unmasked = tf.math.multiply(transformation, 1-mask)\n",
        "  difference = tf.subtract(x_unmasked, transformation_unmasked)\n",
        "  difference = tf.reshape(difference, [-1,784,1])\n",
        "  return tf.math.square(tf.norm(difference, ord=2, axis=1))\n",
        "#loss_const(recon_image, trafo_image, mask_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOPgqnsS3MDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# kappa is the expected rate of changing pixels, a hyperparameter\n",
        "def loss_count(mask, kappa):\n",
        "  return tf.math.maximum(tf.math.reduce_sum(abs(mask), axis=[1,2])/784, kappa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR39ElExYb17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_smoothness(mask):\n",
        "  return tf.image.total_variation(mask)\n",
        "#loss_smoothness(recon_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SyydvEioXfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_entropy(mask):\n",
        "  min_elementwise = tf.minimum(mask, 1-mask)\n",
        "  min_elementwise = tf.reshape(min_elementwise, [-1,784,1])\n",
        "  return tf.norm(min_elementwise, ord=2, axis=1)\n",
        "#testing = tf.reshape(tf.minimum(trafo_image, 1-trafo_image), [-1,784,1])\n",
        "#tf.reduce_sum(tf.math.square(testing))\n",
        "#tf.math.square(tf.norm(testing, ord=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXwVkoxkqIK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TESTING FIELD 2.0 -> Used to change stuff in the training step. The most up-to-date version is in the training step\n",
        "images = test_images\n",
        "predicted_classes = classifier.predict(images)\n",
        "predicted_classes = np.argmax(predicted_classes, axis=1) # now we have the actual predictions\n",
        "predicted_classes = tf.reshape(predicted_classes, [-1,1]) # make it compatible with tensors below\n",
        "predicted_classes = tf.cast(predicted_classes, tf.float32) # change type to float for multiplications\n",
        "\n",
        "# More components (8 to be precise) in this network than in DCGAN -> Needs more gradient tapes\n",
        "# List: enc4_tape, enc9_tape, gen_start_tape, recon_tape, trafo_tape, disc4_tape, disc9_tape\n",
        "with tf.GradientTape() as enc4_tape, tf.GradientTape() as enc9_tape, tf.GradientTape() as gen_start_tape, tf.GradientTape() as recon_tape, tf.GradientTape() as trafo_tape, tf.GradientTape() as mask_tape, tf.GradientTape() as disc4_tape, tf.GradientTape() as disc9_tape:\n",
        "  # Put all images through _both_ streams. Only use one of them for each image for loss computation\n",
        "  z_as4 = encoder4(images)\n",
        "  z_as9 = encoder9(images)\n",
        "  # 'middle step', apply the part of the generator shared by trafo and recon\n",
        "  gen_from_pred4 = generator_start(z_as4)\n",
        "  gen_from_pred9 = generator_start(z_as9)\n",
        "  # reconstructions, transformations and masks, then create composites\n",
        "  recon_from_pred4 = reconstructor(gen_from_pred4)\n",
        "  recon_from_pred9 = reconstructor(gen_from_pred9)\n",
        "  trafo_from_pred4 = transformator(gen_from_pred4)\n",
        "  trafo_from_pred9 = transformator(gen_from_pred9)\n",
        "  mask_from_pred4 = mask(gen_from_pred4)\n",
        "  mask_from_pred9 = mask(gen_from_pred9) \n",
        "  comp_from_pred4 = create_composite(images, trafo_from_pred4, mask_from_pred4)\n",
        "  comp_from_pred9 = create_composite(images, trafo_from_pred9, mask_from_pred9)\n",
        "  \n",
        "  # now we need to get the losses right\n",
        "  # GAN loss - first create all necessary discriminator outputs\n",
        "  real_output4 = discriminator4(images)\n",
        "  real_output9 = discriminator9(images)\n",
        "  recon_output4 = discriminator4(recon_from_pred4)\n",
        "  recon_output9 = discriminator9(recon_from_pred9)\n",
        "  trafo_output4 = discriminator9(trafo_from_pred4) # now use the opposite discriminators\n",
        "  trafo_output9 = discriminator4(trafo_from_pred9)\n",
        "  comp_output4 = discriminator9(comp_from_pred4)\n",
        "  comp_output9 = discriminator4(comp_from_pred9)\n",
        "  # now calculate the loss (split up by predicted class, not by produced class as in Silberman's paper)\n",
        "  loss_gan_pred4 = loss_gan(real_output4, recon_output4, trafo_output4, comp_output4)\n",
        "  loss_gan_pred9 = loss_gan(real_output9, recon_output9, trafo_output9, comp_output9)\n",
        "  loss_g4 = tf.math.multiply(1-predicted_classes, loss_gan_pred4)\n",
        "  loss_g9 = tf.math.multiply(predicted_classes, loss_gan_pred9)\n",
        "  loss_g = loss_g4 + loss_g9\n",
        "  \n",
        "  # classifier loss - first we need to create predictions for our composite images\n",
        "  pred_comp_from_pred4 = classifier(comp_from_pred4)\n",
        "  pred_comp_from_pred4 = pred_comp_from_pred4[:,1] # not argmax here, we need the probability of a 9\n",
        "  pred_comp_from_pred4 = tf.reshape(pred_comp_from_pred4, [-1,1])\n",
        "  #pred_comp_from_pred4 = tf.cast(pred_comp_from_pred4, tf.float32)\n",
        "  pred_comp_from_pred9 = classifier(comp_from_pred9)\n",
        "  pred_comp_from_pred9 = pred_comp_from_pred9[:,1] # not argmax here, we need the probability\n",
        "  pred_comp_from_pred9 = tf.reshape(pred_comp_from_pred9, [-1,1])\n",
        "  #pred_comp_from_pred9 = tf.cast(pred_comp_from_pred9, tf.float32)\n",
        "  # now calculate the loss\n",
        "  loss_class_pred4 = loss_classifier4(pred_comp_from_pred4) # put a composite 9 into the loss_c for predictions 4\n",
        "  loss_class_pred9 = loss_classifier9(pred_comp_from_pred9)\n",
        "  loss_c4 = tf.math.multiply(1-predicted_classes, loss_class_pred4)\n",
        "  loss_c9 = tf.math.multiply(predicted_classes, loss_class_pred9)\n",
        "  loss_c = loss_c4 + loss_c9\n",
        "  \n",
        "  # reconstruction loss loss_r\n",
        "  loss_recon4 = loss_recon(images, recon_from_pred4)\n",
        "  loss_recon9 = loss_recon(images, recon_from_pred9)\n",
        "  loss_r4 = tf.math.multiply(1-predicted_classes, loss_recon4) # set the loss for the wrong recons to 0\n",
        "  loss_r9 = tf.math.multiply(predicted_classes, loss_recon9) # set the loss for the wrong recons to 0\n",
        "  loss_r = loss_r4 + loss_r9\n",
        "\n",
        "  # the 4 prior losses, try kappa=0.03 in loss_count\n",
        "  kappa = 0.03\n",
        "  loss_const4 = loss_const(images, trafo_from_pred4, mask_from_pred4)\n",
        "  loss_const9 = loss_const(images, trafo_from_pred9, mask_from_pred9)\n",
        "  loss_cs4 = tf.math.multiply(1-predicted_classes, loss_const4)\n",
        "  loss_cs9 = tf.math.multiply(predicted_classes, loss_const9)\n",
        "  loss_cs = loss_cs4 + loss_cs9\n",
        "  loss_count4 = loss_count(mask_from_pred4, kappa)\n",
        "  loss_count9 = loss_count(mask_from_pred9, kappa)\n",
        "  loss_ct4 = tf.math.multiply(1-predicted_classes, loss_count4)\n",
        "  loss_ct9 = tf.math.multiply(predicted_classes, loss_count9)\n",
        "  loss_ct = loss_ct4 + loss_ct9\n",
        "  loss_smooth4 = loss_smoothness(mask_from_pred4)\n",
        "  loss_smooth9 = loss_smoothness(mask_from_pred9)\n",
        "  loss_sm4 = tf.math.multiply(1-predicted_classes, loss_smooth4)\n",
        "  loss_sm9 = tf.math.multiply(predicted_classes, loss_smooth9)\n",
        "  loss_sm = loss_sm4 + loss_sm9\n",
        "  loss_entropy4 = loss_entropy(mask_from_pred4)\n",
        "  loss_entropy9 = loss_entropy(mask_from_pred9)\n",
        "  loss_en4 = tf.math.multiply(1-predicted_classes, loss_entropy4)\n",
        "  loss_en9 = tf.math.multiply(predicted_classes, loss_entropy9)\n",
        "  loss_en = loss_en4 + loss_en9\n",
        "\n",
        "  loss_prior = loss_cs + loss_ct + loss_sm + loss_en\n",
        "\n",
        "#print(tf.reduce_max(loss_en), tf.reduce_min(loss_en))\n",
        "print(tf.reduce_max(comp_output4), tf.reduce_min(comp_output4))\n",
        "#tf.math.reduce_sum(abs(mask_from_pred4), axis=[1,2])\n",
        "#tf.norm(mask_from_pred4, ord=1)\n",
        "#print(tf.reduce_max(pred_comp_from_pred9), tf.reduce_min(pred_comp_from_pred9))\n",
        "#tf.math.log(9.70742e-13) = -27 which is harmless\n",
        "#print(tf.reduce_max(loss_c), tf.reduce_min(loss_c))\n",
        "#plt.imshow(comp_from_pred4[0, :, :, 0] * 127.5 + 127.5, cmap='gray') # still looks very much like the original image since mask is close to 0\n",
        "#plt.imshow(images[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "#print(tf.reduce_max(mask_from_pred4), tf.reduce_min(mask_from_pred4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MgIc7i0th_Iu"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "DCGAN: Use one optimizer for discriminator and one for the generator\n",
        "\n",
        "How about ExplainGAN? We have a bunch of networks. What will be trained separately and what not? Let's try one optimizer for each network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "source": [
        "# Add optimizer for all the different parts: enc4, enc9, gen_start, recon, trafo, disc4, disc9\n",
        "enc4_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "enc9_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "gen_start_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "recon_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "trafo_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "mask_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc4_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc9_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "# I could change the Adam parameter. The CDGAN paper suggested 2e-4 instead of 1e-4. It also suggested to change another parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Define the training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jylSonrqSWfi"
      },
      "source": [
        "Go through the whole architecture in one training step. As input use a selection of real images, it is not important how many of each class we take. At the start of the process, the classifier puts predicted labels on each image. Then, all pictures go through the other parts of the network twice, once for each possible predicted class. When computing the losses at the end, only use the actual predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3t5ibNo05jCB",
        "colab": {}
      },
      "source": [
        "# From DCGAN original: Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "#@tf.function # produces an error in my code, if I leave it away there is no problem\n",
        "def train_step(images): # images is the whole batch of real images fed into the machine in one training step\n",
        "    predicted_classes = classifier.predict(images)\n",
        "    predicted_classes = np.argmax(predicted_classes, axis=1) # now we have the actual predictions\n",
        "    predicted_classes = tf.reshape(predicted_classes, [-1,1]) # make it compatible with tensors below\n",
        "    predicted_classes = tf.cast(predicted_classes, tf.float32) # change type to float for multiplications\n",
        "    \n",
        "    # More components (8 to be precise) in this network than in DCGAN -> Needs more gradient tapes\n",
        "    # List: enc4_tape, enc9_tape, gen_start_tape, recon_tape, trafo_tape, disc4_tape, disc9_tape\n",
        "    with tf.GradientTape() as enc4_tape, tf.GradientTape() as enc9_tape, tf.GradientTape() as gen_start_tape, tf.GradientTape() as recon_tape, tf.GradientTape() as trafo_tape, tf.GradientTape() as mask_tape, tf.GradientTape() as disc4_tape, tf.GradientTape() as disc9_tape:\n",
        "      # Put all images through _both_ streams. Only use one of them for each image for loss computation\n",
        "      z_as4 = encoder4(images)\n",
        "      z_as9 = encoder9(images)\n",
        "      # 'middle step', apply the part of the generator shared by trafo, recon and mask\n",
        "      gen_from_pred4 = generator_start(z_as4)\n",
        "      gen_from_pred9 = generator_start(z_as9)\n",
        "      # reconstructions, transformations and masks, then create composites\n",
        "      recon_from_pred4 = reconstructor(gen_from_pred4)\n",
        "      recon_from_pred9 = reconstructor(gen_from_pred9)\n",
        "      trafo_from_pred4 = transformator(gen_from_pred4)\n",
        "      trafo_from_pred9 = transformator(gen_from_pred9)\n",
        "      mask_from_pred4 = mask(gen_from_pred4)\n",
        "      mask_from_pred9 = mask(gen_from_pred9) \n",
        "      comp_from_pred4 = create_composite(images, trafo_from_pred4, mask_from_pred4)\n",
        "      comp_from_pred9 = create_composite(images, trafo_from_pred9, mask_from_pred9)\n",
        "      \n",
        "      # loss weights: Scale everything to be close to 3 (leave the only negative loss, GAN loss, which is around -2.8 untouched)\n",
        "      # I might need to change a lot here. No idea about the weights used by Silberman\n",
        "      weight_c = 0.33\n",
        "      weight_r = 0.004\n",
        "      weight_cs = 0.004\n",
        "      weight_ct = 300\n",
        "      weight_sm = 0.15\n",
        "      weight_en = 8.5\n",
        "\n",
        "      # now we need to get the losses right\n",
        "      # GAN loss - first create all necessary discriminator outputs\n",
        "      real_output4 = discriminator4(images)\n",
        "      real_output9 = discriminator9(images)\n",
        "      recon_output4 = discriminator4(recon_from_pred4)\n",
        "      recon_output9 = discriminator9(recon_from_pred9)\n",
        "      trafo_output4 = discriminator9(trafo_from_pred4) # now use the opposite discriminators\n",
        "      trafo_output9 = discriminator4(trafo_from_pred9)\n",
        "      comp_output4 = discriminator9(comp_from_pred4)\n",
        "      comp_output9 = discriminator4(comp_from_pred9)\n",
        "      # now calculate the loss (split up by predicted class, not by produced class as in Silberman's paper)\n",
        "      #loss_gan_p9_real = loss_gan_real(real_output9)\n",
        "      #loss_gan_p9_recon = loss_gan_recon(recon_output9)\n",
        "      #loss_gan_p9_trafo = loss_gan_trafo(trafo_output9)\n",
        "      #loss_gan_p9_comp = loss_gan_comp(comp_output9)\n",
        "      #loss_gan_pred9 = loss_gan_p9_real + loss_gan_p9_recon + loss_gan_p9_trafo + loss_gan_p9_comp\n",
        "      loss_gan_pred4 = loss_gan(real_output4, recon_output4, trafo_output4, comp_output4)\n",
        "      loss_gan_pred9 = loss_gan(real_output9, recon_output9, trafo_output9, comp_output9)\n",
        "      loss_g4 = tf.math.multiply(1-predicted_classes, loss_gan_pred4)\n",
        "      loss_g9 = tf.math.multiply(predicted_classes, loss_gan_pred9)\n",
        "      loss_g = loss_g4 + loss_g9\n",
        "      \n",
        "      # classifier loss - first we need to create predictions for our composite images\n",
        "      pred_comp_from_pred4 = classifier(comp_from_pred4)\n",
        "      pred_comp_from_pred4 = pred_comp_from_pred4[:,1] # not argmax here, we need the probability of a 9\n",
        "      pred_comp_from_pred4 = tf.reshape(pred_comp_from_pred4, [-1,1])\n",
        "      pred_comp_from_pred4 = tf.cast(pred_comp_from_pred4, tf.float32)\n",
        "      pred_comp_from_pred9 = classifier(comp_from_pred9)\n",
        "      pred_comp_from_pred9 = pred_comp_from_pred9[:,1] # not argmax here, we need the probability\n",
        "      pred_comp_from_pred9 = tf.reshape(pred_comp_from_pred9, [-1,1])\n",
        "      pred_comp_from_pred9 = tf.cast(pred_comp_from_pred9, tf.float32)\n",
        "      # now calculate the loss\n",
        "      loss_class_pred4 = loss_classifier4(pred_comp_from_pred4) # put a composite 9 into the loss_c for predictions 4\n",
        "      loss_class_pred9 = loss_classifier9(pred_comp_from_pred9)\n",
        "      loss_c4 = tf.math.multiply(1-predicted_classes, loss_class_pred4)\n",
        "      loss_c9 = tf.math.multiply(predicted_classes, loss_class_pred9)\n",
        "      loss_c = (loss_c4 + loss_c9) * weight_c\n",
        "      \n",
        "      # reconstruction loss loss_r\n",
        "      loss_recon4 = loss_recon(images, recon_from_pred4)\n",
        "      loss_recon9 = loss_recon(images, recon_from_pred9)\n",
        "      loss_r4 = tf.math.multiply(1-predicted_classes, loss_recon4) # set the loss for the wrong recons to 0\n",
        "      loss_r9 = tf.math.multiply(predicted_classes, loss_recon9) # set the loss for the wrong recons to 0\n",
        "      loss_r = (loss_r4 + loss_r9) * weight_r\n",
        "\n",
        "      # the 4 prior losses, try kappa=0.03 in loss_count\n",
        "      kappa = 0.03\n",
        "      loss_const4 = loss_const(images, trafo_from_pred4, mask_from_pred4)\n",
        "      loss_const9 = loss_const(images, trafo_from_pred9, mask_from_pred9)\n",
        "      loss_cs4 = tf.math.multiply(1-predicted_classes, loss_const4)\n",
        "      loss_cs9 = tf.math.multiply(predicted_classes, loss_const9)\n",
        "      loss_cs = (loss_cs4 + loss_cs9) * weight_cs\n",
        "      loss_count4 = loss_count(mask_from_pred4, kappa)\n",
        "      loss_count9 = loss_count(mask_from_pred9, kappa)\n",
        "      loss_ct4 = tf.math.multiply(1-predicted_classes, loss_count4)\n",
        "      loss_ct9 = tf.math.multiply(predicted_classes, loss_count9)\n",
        "      loss_ct = (loss_ct4 + loss_ct9) * weight_ct\n",
        "      loss_smooth4 = loss_smoothness(mask_from_pred4)\n",
        "      loss_smooth9 = loss_smoothness(mask_from_pred9)\n",
        "      loss_sm4 = tf.math.multiply(1-predicted_classes, loss_smooth4)\n",
        "      loss_sm9 = tf.math.multiply(predicted_classes, loss_smooth9)\n",
        "      loss_sm = (loss_sm4 + loss_sm9) * weight_sm\n",
        "      loss_entropy4 = loss_entropy(mask_from_pred4)\n",
        "      loss_entropy9 = loss_entropy(mask_from_pred9)\n",
        "      loss_en4 = tf.math.multiply(1-predicted_classes, loss_entropy4)\n",
        "      loss_en9 = tf.math.multiply(predicted_classes, loss_entropy9)\n",
        "      loss_en = (loss_en4 + loss_en9) * weight_en\n",
        "\n",
        "      # Add up losses that are used together\n",
        "      loss_summed = loss_g + loss_c + loss_r \n",
        "      loss_prior = loss_cs + loss_ct + loss_sm + loss_en\n",
        "\n",
        "      # Mask uses the prior losses only and discriminators use GAN loss only. The rest uses GAN, classifier and recon loss  \n",
        "      gradients_of_enc4 = enc4_tape.gradient(loss_summed, encoder4.trainable_variables)\n",
        "      gradients_of_enc9 = enc9_tape.gradient(loss_summed, encoder9.trainable_variables)\n",
        "      gradients_of_gen_start = gen_start_tape.gradient(loss_summed, generator_start.trainable_variables)\n",
        "      gradients_of_recon = recon_tape.gradient(loss_summed, reconstructor.trainable_variables)\n",
        "      gradients_of_trafo = trafo_tape.gradient(loss_summed, transformator.trainable_variables)\n",
        "      gradients_of_mask = mask_tape.gradient(loss_prior, mask.trainable_variables)\n",
        "      gradients_of_disc4 = disc4_tape.gradient(-loss_g, discriminator4.trainable_variables)\n",
        "      gradients_of_disc9 = disc9_tape.gradient(-loss_g, discriminator9.trainable_variables)\n",
        "\n",
        "      #enc4_optimizer.apply_gradients(zip(gradients_of_enc4, encoder4.trainable_variables))\n",
        "      #enc9_optimizer.apply_gradients(zip(gradients_of_enc9, encoder9.trainable_variables))\n",
        "      #gen_start_optimizer.apply_gradients(zip(gradients_of_gen_start, generator_start.trainable_variables))\n",
        "      #recon_optimizer.apply_gradients(zip(gradients_of_recon, reconstructor.trainable_variables))\n",
        "      #trafo_optimizer.apply_gradients(zip(gradients_of_trafo, transformator.trainable_variables))\n",
        "      #mask_optimizer.apply_gradients(zip(gradients_of_mask, mask.trainable_variables))\n",
        "      disc4_optimizer.apply_gradients(zip(gradients_of_disc4, discriminator4.trainable_variables))      \n",
        "      disc9_optimizer.apply_gradients(zip(gradients_of_disc9, discriminator9.trainable_variables))\n",
        "\n",
        "      print('###################################################################')\n",
        "      #print('z_as4: ', tf.reduce_max(z_as4), tf.reduce_min(z_as4))\n",
        "      #print('gen_from_pred4: ', tf.reduce_max(gen_from_pred4), tf.reduce_min(gen_from_pred4))\n",
        "      #print('recon_from_pred4: ', tf.reduce_max(recon_from_pred4), tf.reduce_min(recon_from_pred4))\n",
        "      print('real_output4: ', tf.reduce_max(real_output4), tf.reduce_min(real_output4))\n",
        "      print('recon_output4: ', tf.reduce_max(recon_output4), tf.reduce_min(recon_output4))\n",
        "      print('trafo_output4: ', tf.reduce_max(trafo_output4), tf.reduce_min(trafo_output4))\n",
        "      print('comp_output4: ', tf.reduce_max(comp_output4), tf.reduce_min(comp_output4), tf.reduce_mean(comp_output4))\n",
        "      #print('comp_output9: ', tf.reduce_max(comp_output9), tf.reduce_min(comp_output9), tf.reduce_mean(comp_output9))\n",
        "      print('loss_g: ', tf.reduce_max(loss_g), tf.reduce_min(loss_g))\n",
        "      #print('loss_gan_real4: ', tf.reduce_max(loss_gan_p4_real), tf.reduce_min(loss_gan_p4_real))\n",
        "      #print('loss_gan_recon4: ', tf.reduce_max(loss_gan_p4_recon), tf.reduce_min(loss_gan_p4_recon))\n",
        "      #print('loss_gan_trafo4: ', tf.reduce_max(loss_gan_p4_trafo), tf.reduce_min(loss_gan_p4_trafo))\n",
        "      #print('loss_gan_comp4: ', tf.reduce_max(loss_gan_p4_comp), tf.reduce_min(loss_gan_p4_comp))\n",
        "      #print('loss_gan_real9: ', tf.reduce_max(loss_gan_p9_real), tf.reduce_min(loss_gan_p9_real))\n",
        "      #print('loss_gan_recon9: ', tf.reduce_max(loss_gan_p9_recon), tf.reduce_min(loss_gan_p9_recon))\n",
        "      #print('loss_gan_trafo9: ', tf.reduce_max(loss_gan_p9_trafo), tf.reduce_min(loss_gan_p9_trafo))\n",
        "      #print('loss_gan_comp9: ', tf.reduce_max(loss_gan_p9_comp), tf.reduce_min(loss_gan_p9_comp))\n",
        "      #print('loss_c: ', tf.reduce_max(loss_c), tf.reduce_min(loss_c))\n",
        "      #print('loss_r: ', tf.reduce_max(loss_r), tf.reduce_min(loss_r))\n",
        "      #print('loss_summed: ', tf.reduce_max(loss_summed), tf.reduce_min(loss_summed))\n",
        "      #print('loss_cs: ', tf.reduce_max(loss_cs), tf.reduce_min(loss_cs))\n",
        "      #print('loss_ct: ', tf.reduce_max(loss_ct), tf.reduce_min(loss_ct))\n",
        "      #print('loss_sm: ', tf.reduce_max(loss_sm), tf.reduce_min(loss_sm))\n",
        "      #print('loss_en: ', tf.reduce_max(loss_en), tf.reduce_min(loss_en))\n",
        "      #print(z_as4.shape, real_output4.shape, loss_g.shape, loss_summed.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXe2p8OMBd5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in  range(10):\n",
        "  #train_step(test_images[0:1000,:,:,:])\n",
        "  #test_images[0:1,:,:,:].shape\n",
        "  #generate_and_save_images(1, train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2M7LmLtGEMQJ",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # Here we use the batching of the dataset below\n",
        "      # call the function train_step defined in the box above this one\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as we go (from DCGAN)\n",
        "    #display.clear_output(wait=True)\n",
        "    generate_and_save_images(epoch + 1, train_images) # still input train_images here. Would be nice to use dataset instead!!\n",
        "    \n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)\n",
        "  generate_and_save_images(epochs, train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2aFF7Hk3XdeW"
      },
      "source": [
        "**Generate and save images**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RmdVsmvhPxyy",
        "colab": {}
      },
      "source": [
        "# Changes compared to the DCGAN version: test_input is the first real image(s) instead of a random seed\n",
        "def generate_and_save_images(epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  original = test_input[0:1,:,:,:]\n",
        "  z_as4 = encoder4(original, training=False)\n",
        "  gen_from_pred4 = generator_start(z_as4, training=False)\n",
        "  recon_from_pred4 = reconstructor(gen_from_pred4, training=False)\n",
        "  trafo_from_pred4 = transformator(gen_from_pred4, training=False)\n",
        "  mask_from_pred4 = mask(gen_from_pred4, training=False)\n",
        "  comp_from_pred4 = create_composite(original, trafo_from_pred4, mask_from_pred4)\n",
        "  \n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  plt.subplot(1, 5, 1)\n",
        "  plt.imshow(original[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 2)\n",
        "  plt.imshow(recon_from_pred4[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 3)\n",
        "  plt.imshow(trafo_from_pred4[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 4)\n",
        "  plt.imshow(mask_from_pred4[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 5)\n",
        "  plt.imshow(comp_from_pred4[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gsckd2SBhAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_and_save_images(1,train_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "## Train the model\n",
        "Call the `train()` method defined above to train the generators and discriminator and the other networks simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
        "\n",
        "DCGAN: At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDdf2OfwxkUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch and shuffle the data in DCGAN - do I need to shuffle?\n",
        "# We can play around with the batch size a bit. Smaller batch sizes make computations faster\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(len(train_labels)).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(len(test_labels)).batch(BATCH_SIZE)\n",
        "EPOCHS = 7\n",
        "num_examples_to_generate = 4 # the gif is not super important. Ignore it for now"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ly3UN0SLLY2l",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "train(test_dataset, EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4M_vIbUi7c0"
      },
      "source": [
        "## Create a GIF (ignored for now)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WfO5wCdclHGL",
        "colab": {}
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "#def display_image(epoch_no):\n",
        "#  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5x3q9_Oe5q0A",
        "colab": {}
      },
      "source": [
        "#display_image(EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NywiH3nL8guF"
      },
      "source": [
        "Use `imageio` to create an animated gif using the images saved during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IGKQgENQ8lEI",
        "colab": {}
      },
      "source": [
        "#anim_file = 'dcgan.gif'\n",
        "\n",
        "#with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "#  filenames = glob.glob('image*.png')\n",
        "#  filenames = sorted(filenames)\n",
        "#  last = -1\n",
        "#  for i,filename in enumerate(filenames):\n",
        "#    frame = 2*(i**0.5)\n",
        "#    if round(frame) > round(last):\n",
        "#      last = frame\n",
        "#    else:\n",
        "#      continue\n",
        "#    image = imageio.imread(filename)\n",
        "#    writer.append_data(image)\n",
        "#  image = imageio.imread(filename)\n",
        "#  writer.append_data(image)\n",
        "\n",
        "#import IPython\n",
        "#if IPython.version_info > (6,2,0,''):\n",
        "#  display.Image(filename=anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGhC3-fMWSwl"
      },
      "source": [
        "If you're working in Colab you can download the animation with the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uV0yiKpzNP1b",
        "colab": {}
      },
      "source": [
        "#try:\n",
        "#  from google.colab import files\n",
        "#except ImportError:\n",
        "#   pass\n",
        "#else:\n",
        "#  files.download(anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k6qC-SbjK0yW"
      },
      "source": [
        "## Next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xjjkT9KAK6H7"
      },
      "source": [
        "This tutorial has shown the complete code necessary to write and train a GAN. As a next step, you might like to experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset/home). To learn more about GANs we recommend the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n"
      ]
    }
  ]
}