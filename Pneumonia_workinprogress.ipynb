{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLsyOht9qJciiA00bs1R3p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarsAmker/ExplainGAN/blob/master/Pneumonia_workinprogress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vs3PZuruFIW",
        "colab_type": "text"
      },
      "source": [
        "# Import and access the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmpTb0PZ50ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziaB1iqz9xgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pwd\n",
        "!mkdir ~/.kaggle\n",
        "#!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwimdKtue5rn",
        "colab_type": "text"
      },
      "source": [
        "The kaggle.json file is needed to get the data directly from kaggle. Copy it over from my google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDU7J7Czx-7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0YOj91hd0vc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy the json file from google drive into the kaggle directory\n",
        "!cp \"/content/gdrive/My Drive/kaggle.json\" ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q85yyv5K7OiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pneumonia data (first line makes kaggle API key unreadable)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "# the zip file is listed in the left hand pane after the download. We need to unzip and create paths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsvFZPXofXpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip chest-xray-pneumonia.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-lNB19g0PM",
        "colab_type": "code",
        "outputId": "acf5178d-23ea-471a-cc04-a3f71638029b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import shutil\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import imgaug.augmenters as iaa\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "# added tensorflow and stuff from the MNIST classifier:\n",
        "import tensorflow as tf\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import PIL\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCWnHofyg9om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My directory is called \"content\" instead of \"input\" as in the original code\n",
        "#os.listdir(\"../content/chest_xray/chest_xray\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXUkM73Hh5Wz",
        "colab_type": "text"
      },
      "source": [
        "## Make the data accessible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvEF_xzy7mL1",
        "colab_type": "text"
      },
      "source": [
        "NAIN had a code box fixing some random seeds here. Not compatible with tf2 however, therefore deleted and saved in testPneumonia/reproducability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi2uV41X7qVs",
        "colab_type": "text"
      },
      "source": [
        "The dataset is divided into three sets: 1) train set 2) validation set and 3) test set. Let's grab the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c57VWXTohsom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define path to the data directory: \"content\" instead of \"input/chest-xray-pneumonia\" as in the original code\n",
        "data_dir = Path('../content/chest_xray/chest_xray')\n",
        "\n",
        "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
        "train_dir = data_dir / 'train'\n",
        "\n",
        "# Path to validation directory\n",
        "val_dir = data_dir / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_dir = data_dir / 'test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hXJKM3M7uLr",
        "colab_type": "text"
      },
      "source": [
        "Each of the above directories contains two sub-directories:\n",
        "\n",
        "NORMAL: These are the samples that describe the normal (no pneumonia) case.\n",
        "\n",
        "PNEUMONIA: This directory contains those samples that are the pneumonia cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxdElOCu6F-",
        "colab_type": "text"
      },
      "source": [
        "### Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNl_MYGri6Pr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the path to the normal and pneumonia sub-directories\n",
        "normal_cases_dir = train_dir / 'NORMAL'\n",
        "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
        "\n",
        "# Get the list of all the images\n",
        "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "\n",
        "# An empty list. We will insert the data into this list in (img_path, label) format\n",
        "train_data = []\n",
        "\n",
        "# Go through all the normal cases. The label for these cases will be 0\n",
        "for img in normal_cases:\n",
        "    train_data.append((img,0))\n",
        "\n",
        "# Go through all the pneumonia cases. The label for these cases will be 1\n",
        "for img in pneumonia_cases:\n",
        "    train_data.append((img, 1))\n",
        "\n",
        "# Get a pandas dataframe from the data we have in our list \n",
        "train_data = pd.DataFrame(train_data, columns=['image', 'label'],index=None)\n",
        "\n",
        "# Shuffle the data \n",
        "train_data = train_data.sample(frac=1.).reset_index(drop=True)\n",
        "\n",
        "# How the dataframe looks like?\n",
        "#train_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyjEp7cX8KtA",
        "colab_type": "text"
      },
      "source": [
        "### Validation data\n",
        "We will be defining a generator for the training dataset later in the notebook but as the validation data is small, so I can read the images and can load the data without the need of a generator. This is exactly what the code block given below is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOAlztNV-Oem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Why do we make artificial RGB pictures? Because the weights from imageNet that we import are for RGB pictures\n",
        "# Get the path to the sub-directories\n",
        "normal_cases_dir = val_dir / 'NORMAL'\n",
        "pneumonia_cases_dir = val_dir / 'PNEUMONIA'\n",
        "\n",
        "# Get the list of all the images\n",
        "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "\n",
        "# List that are going to contain validation images data and the corresponding labels\n",
        "valid_data = []\n",
        "valid_labels = []\n",
        "\n",
        "# Some images are in grayscale while majority of them contains 3 channels. \n",
        "# So, if the image is grayscale, we will convert into a image with 3 channels.\n",
        "# We will normalize the pixel values and resizing all the images to 224x224 \n",
        "\n",
        "# Normal cases\n",
        "for img in normal_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img]) # add a third dimension for RGB channels and fill it with 3 copies of the original\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(0, num_classes=2)\n",
        "    valid_data.append(img)\n",
        "    valid_labels.append(label)\n",
        "                      \n",
        "# Pneumonia cases        \n",
        "for img in pneumonia_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(1, num_classes=2)\n",
        "    valid_data.append(img)\n",
        "    valid_labels.append(label)\n",
        "    \n",
        "# Convert the list into numpy arrays\n",
        "valid_data = np.array(valid_data)\n",
        "valid_labels = np.array(valid_labels)\n",
        "\n",
        "#print(\"Total number of validation examples: \", valid_data.shape)\n",
        "#print(\"Total number of labels:\", valid_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RREnBO0EvMEH",
        "colab_type": "text"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Pynl6EfAI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing test data\n",
        "normal_cases_dir = test_dir / 'NORMAL'\n",
        "pneumonia_cases_dir = test_dir / 'PNEUMONIA'\n",
        "\n",
        "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
        "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
        "\n",
        "test_data = []\n",
        "test_labels = []\n",
        "\n",
        "for img in normal_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(0, num_classes=2)\n",
        "    test_data.append(img)\n",
        "    test_labels.append(label)\n",
        "                      \n",
        "for img in pneumonia_cases:\n",
        "    img = cv2.imread(str(img))\n",
        "    img = cv2.resize(img, (224,224))\n",
        "    if img.shape[2] ==1:\n",
        "        img = np.dstack([img, img, img])\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)/255.\n",
        "    label = to_categorical(1, num_classes=2)\n",
        "    test_data.append(img)\n",
        "    test_labels.append(label)\n",
        "\n",
        "test_data = np.array(test_data)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "#print(\"Total number of test examples: \", test_data.shape)\n",
        "#print(\"Total number of labels:\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr_2t1c9-cmG",
        "colab_type": "text"
      },
      "source": [
        "### Training data generator (stop ExplainGAN preprocessing before this?)\n",
        "Data augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. But augmentation can be much more helpful where the dataset is imbalanced. You can generate different samples of undersampled class in order to try to balance the overall distribution.\n",
        "I like imgaug a lot. It comes with a very clean api and you can do hell of augmentations with it. It's worth exploring!! In the next code block, I will define a augmentation sequence. You will notice Oneof and it does exactly that. At each iteration, it will take one augmentation technique out of the three and will apply that on the samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3vBbZ_c-nMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Augmentation sequence \n",
        "seq = iaa.OneOf([\n",
        "    iaa.Fliplr(), # horizontal flips\n",
        "    iaa.Affine(rotate=20), # roatation\n",
        "    iaa.Multiply((1.2, 1.5))]) #random brightness"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EawVbLY-xz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(data, batch_size):\n",
        "    # Get total number of samples in the data\n",
        "    n = len(data)\n",
        "    steps = n//batch_size\n",
        "    \n",
        "    # Define two numpy arrays for containing batch data and labels\n",
        "    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n",
        "    batch_labels = np.zeros((batch_size,2), dtype=np.float32)\n",
        "\n",
        "    # Get a numpy array of all the indices of the input data\n",
        "    indices = np.arange(n)\n",
        "    \n",
        "    # Initialize a counter\n",
        "    i =0\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        # Get the next batch \n",
        "        count = 0\n",
        "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
        "        for j, idx in enumerate(next_batch):\n",
        "            img_name = data.iloc[idx]['image']\n",
        "            label = data.iloc[idx]['label']\n",
        "            \n",
        "            # one hot encoding\n",
        "            encoded_label = to_categorical(label, num_classes=2)\n",
        "            # read the image and resize\n",
        "            img = cv2.imread(str(img_name))\n",
        "            img = cv2.resize(img, (224,224))\n",
        "            \n",
        "            # check if it's grayscale\n",
        "            if img.shape[2]==1:\n",
        "                img = np.dstack([img, img, img])\n",
        "            \n",
        "            # cv2 reads in BGR mode by default\n",
        "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            # normalize the image pixels\n",
        "            orig_img = img.astype(np.float32)/255.\n",
        "            \n",
        "            batch_data[count] = orig_img\n",
        "            batch_labels[count] = encoded_label\n",
        "            \n",
        "            # generating more samples of the undersampled class\n",
        "            if label==0 and count < batch_size-2:\n",
        "                aug_img1 = seq.augment_image(img)\n",
        "                aug_img2 = seq.augment_image(img)\n",
        "                aug_img1 = cv2.cvtColor(aug_img1, cv2.COLOR_BGR2RGB)\n",
        "                aug_img2 = cv2.cvtColor(aug_img2, cv2.COLOR_BGR2RGB)\n",
        "                aug_img1 = aug_img1.astype(np.float32)/255.\n",
        "                aug_img2 = aug_img2.astype(np.float32)/255.\n",
        "\n",
        "                batch_data[count+1] = aug_img1\n",
        "                batch_labels[count+1] = encoded_label\n",
        "                batch_data[count+2] = aug_img2\n",
        "                batch_labels[count+2] = encoded_label\n",
        "                count +=2\n",
        "            \n",
        "            else:\n",
        "                count+=1\n",
        "            \n",
        "            if count==batch_size-1:\n",
        "                break\n",
        "            \n",
        "        i+=1\n",
        "        yield batch_data, batch_labels\n",
        "            \n",
        "        if i>=steps:\n",
        "            i=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLX5Kddl-zhj",
        "colab_type": "text"
      },
      "source": [
        "# Classifier (NAIN's depthwise kernel)\n",
        "\n",
        "Build and compile an empty model and then load the weights for it from my google drive. These weights were created and saved in the notebook \"PneumoniaClassifier.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j76t5VFIjUwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# changed the format of this method due to compatibility with tensorflow 2\n",
        "# The original version from NAIN worked with \"x=...(x)\" steps, my theory is that this x is the placeholder that lead to errors\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv1_1', input_shape=[224, 224, 3]))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv1_2', input_shape=[224, 224, 3]))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool'))\n",
        "\n",
        "  model.add(layers.SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1'))\n",
        "  model.add(layers.SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2'))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool2'))\n",
        "\n",
        "  model.add(layers.SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1'))\n",
        "  model.add(layers.BatchNormalization(name='bn1'))\n",
        "  model.add(layers.SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2'))\n",
        "  model.add(layers.BatchNormalization(name='bn2'))\n",
        "  model.add(layers.SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3'))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool3'))\n",
        "\n",
        "  model.add(layers.SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1'))\n",
        "  model.add(layers.BatchNormalization(name='bn3'))\n",
        "  model.add(layers.SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2'))\n",
        "  model.add(layers.BatchNormalization(name='bn4'))\n",
        "  model.add(layers.SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3'))\n",
        "  model.add(layers.MaxPooling2D((2,2), name='pool4'))\n",
        "\n",
        "  model.add(layers.Flatten(name='flatten'))\n",
        "  model.add(layers.Dense(1024, activation='relu', name='fc1'))\n",
        "  model.add(layers.Dropout(0.7, name='dropout1'))\n",
        "  model.add(layers.Dense(512, activation='relu', name='fc2'))\n",
        "  model.add(layers.Dropout(0.5, name='dropout2'))\n",
        "  model.add(layers.Dense(2, activation='softmax', name='fc3'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4K3k3vn_Roz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier =  build_model()\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI5o5UGccsam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
        "es = EarlyStopping(patience=5)\n",
        "chkpt = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)\n",
        "classifier.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GsrqKL3qxNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.load_weights('/content/gdrive/My Drive/Colab Notebooks/DepthwiseWeights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP9zfa7QwK9l",
        "colab_type": "text"
      },
      "source": [
        "### Testing classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waOSOfxifEIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation on test dataset\n",
        "test_loss, test_score = classifier.evaluate(test_data, test_labels, batch_size=16)\n",
        "print(\"Loss on test set: \", test_loss)\n",
        "print(\"Accuracy on test set: \", test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thif7JOcfG0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get predictions\n",
        "preds = classifier.predict(test_data, batch_size=16)\n",
        "preds = np.argmax(preds, axis=-1)\n",
        "\n",
        "# Original labels\n",
        "orig_test_labels = np.argmax(test_labels, axis=-1)\n",
        "\n",
        "#print(orig_test_labels.shape)\n",
        "#print(preds.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0BKRPbUfO6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(orig_test_labels, preds)\n",
        "plt.figure()\n",
        "#plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, alpha=0.7,cmap=plt.cm.Blues)\n",
        "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\n",
        "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0GR4luFfU2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate Precision and Recall\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "#print(\"Recall of the classifier is {:.2f}\".format(recall))\n",
        "#print(\"Precision of the classifier is {:.2f}\".format(precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7_2_guHuO-q",
        "colab_type": "text"
      },
      "source": [
        "Running everything up to this point takes 3 and a half minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgKszYAttUBJ",
        "colab_type": "text"
      },
      "source": [
        "# ExplainGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqBEqQP-sJ3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data.shape) # this is a data frame of paths to the images and their labels.\n",
        "# The train data generator gets the images from the paths. This saves memory compared to saving all training images\n",
        "print(valid_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwiRAo4WvfUu",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5BQCqxYveIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copied from https://blog.keras.io/building-autoencoders-in-keras.html, added flattening at the end\n",
        "# Also changed the activation function from relu to sigmoid to not get exploding latent variables\n",
        "# Maybe change this back if I train the encoder separately before training the rest of ExplainGAN. Then, also move the flattening\n",
        "\n",
        "# Change input shape, but use only one of the three RGB channels\n",
        "def make_encoder_model(activ_fct):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(16, (3, 3), activation=activ_fct, padding='same', use_bias=True,\n",
        "                                     input_shape=[224, 224, 3]))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "\n",
        "    model.add(layers.Conv2D(8, (3, 3), activation=activ_fct, padding='same', use_bias=True))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "\n",
        "    model.add(layers.Conv2D(8, (3, 3), activation=activ_fct, padding='same', use_bias=True))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Conv2D(8, (3, 3), activation=activ_fct, padding='same', use_bias=True))\n",
        "    model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    return model\n",
        "# At this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
        "# The convolutions here don't decrease the 2D dimension because they have default (1,1) strides. The MaxPooling does"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfeg_e_j5eGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder0 = make_encoder_model(activ_fct='relu')\n",
        "encoder1 = make_encoder_model(activ_fct='relu')\n",
        "#encoder0.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CWE6TBANf-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = encoder0(test_data[0:1,:,:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBhiqjQpRpVu",
        "colab_type": "text"
      },
      "source": [
        "## Generators\n",
        "\n",
        "The generator_start is the biggest part of ExplainGAN, i.e. the one with the most weights. I already ran into one OOM error with this version of the code after building ~5 models with 123 millions of weights. The vast majority of weights appears in the very first, dense, layer. \n",
        "\n",
        "Make this smaller somehow. Let's try to create fewer 7*7 images (in MNIST, we created 128 of these from an encoded vector of length 128). Try 128 for now. Then we have only 10 millions of weights in generator_start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GODGYc5xBfB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This generator is based on the DCGAN generator. But now, we need 3 outputs and not just one!\n",
        "# Therefore tear it apart in the middle. We will use the second part three times to get recon and trafo and mask \n",
        "def make_generator_model_start():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*128, use_bias=False, input_shape=(1568,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Create images of size 7*7 with 128 channels, all connected to the 128 nodes of the encoded original image\n",
        "    # In the next 2 lines, the tensor is actually reshaped into that channel form\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 128)))\n",
        "    assert model.output_shape == (None, 7, 7, 128) # Note: None is the batch size\n",
        "\n",
        "    # now move 5*5*128(channels) filters over the 7*7*128(channels) images in 1,1 strides. Do this for each of the 64 output channels.\n",
        "    # 64 is the number of different filters we apply. Because of the big number of input channels, each filter is already huge\n",
        "    # The number of parameters here is 5*5*128 (filter weights) *64 (number of filters)\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    return model\n",
        "    \n",
        " \n",
        "# Now the second part. Apply this three times to get reconstruction and transformation and mask\n",
        "# Because of the splitting, I also needed to add the input shape in the first layer of this second part.\n",
        "def make_generator_model_end_sigmoid(): # second version to get a mask with values in [0,1] instead of [-1,1] with tanh\n",
        "    # By taking strides of 2, the size of the image gets doubled in length and width.\n",
        "    # This is the case, because we do a backwards convolution. If we get a 7*7 image by taking (2,2)-strides, we must have started with 14*14\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(7,7,64)))\n",
        "    assert model.output_shape == (None, 14, 14, 32)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(14,14,32)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(28,28,16)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(56,56,8)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='sigmoid'))\n",
        "    assert model.output_shape == (None, 224, 224, 1)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oo_Sb8JDCsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_start = make_generator_model_start()\n",
        "#generator_start.summary()\n",
        "reconstructor = make_generator_model_end_sigmoid()\n",
        "#reconstructor.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QKFRV-mPd1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_started = generator_start(encoded)\n",
        "#gen_started # shape (1,7,7,64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdBMmB8oQWAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reconstructed = reconstructor(gen_started)\n",
        "#reconstructed # shape (1,224,224,1) like a grayscale x-ray scan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4qrO7LlDQqH",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYLx34l5ydUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inputs of loss_recon are two images, the original one and its reconstruction \n",
        "def loss_recon(x, reconstruction):\n",
        "  difference = tf.subtract(x,reconstruction)\n",
        "  # reshape 224*224 to one vector in order to apply the l2 norm to it\n",
        "  # The -1 in the first dimension make sure that the number of images stays the same, images shall not be combined\n",
        "  difference = tf.reshape(difference, [-1,50176,1])\n",
        "  return tf.math.square(tf.norm(difference, ord=2, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O16Ko-9wyrjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing\n",
        "#loss_recon(valid_data[:,:,:,0], valid_data[:,:,:,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoshNbH4F2Sp",
        "colab_type": "text"
      },
      "source": [
        "## Experimenation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfbpRY-C0zJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = valid_data\n",
        "predicted_classes = classifier.predict(images) # model takes (224,224,3) as input\n",
        "predicted_classes = np.argmax(predicted_classes, axis=1) # now we have the actual predictions\n",
        "predicted_classes = tf.reshape(predicted_classes, [-1,1]) # make it compatible with tensors below\n",
        "predicted_classes = tf.cast(predicted_classes, tf.float32) # change type to float for multiplications\n",
        "predicted_classes\n",
        "\n",
        "images_grayscale = images[:,:,:,0:1]\n",
        "#images_grayscale.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-qqZskMBpr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc0_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "enc1_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "gen_start_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "recon_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOp_zlzf3MXq",
        "colab_type": "code",
        "outputId": "92680a15-6a20-4699-e689-582b2bba8f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "with tf.GradientTape() as enc0_tape, tf.GradientTape() as enc1_tape, tf.GradientTape() as gen_start_tape, tf.GradientTape() as recon_tape:\n",
        "  z_as0 = encoder0(images)\n",
        "  z_as1 = encoder1(images)\n",
        "  gen_from_pred0 = generator_start(z_as0)\n",
        "  gen_from_pred1 = generator_start(z_as1)\n",
        "  recon_from_pred0 = reconstructor(gen_from_pred0)\n",
        "  recon_from_pred1 = reconstructor(gen_from_pred1)\n",
        "  print(\"recon0 shape:\", recon_from_pred0.shape)\n",
        "  print(\"recon1 shape:\", recon_from_pred1.shape)\n",
        "  print(\"images_grayscale shape:\", images_grayscale.shape)\n",
        "\n",
        "  loss_recon0 = loss_recon(images_grayscale, recon_from_pred0)\n",
        "  loss_recon1 = loss_recon(images_grayscale, recon_from_pred1)\n",
        "  print(\"loss_recon0 shape:\", loss_recon0.shape)\n",
        "  loss_r0 = tf.math.multiply(1-predicted_classes, loss_recon0) # set the loss for the wrong recons to 0\n",
        "  loss_r1 = tf.math.multiply(predicted_classes, loss_recon1) # set the loss for the wrong recons to 0\n",
        "  loss_r = (loss_r0 + loss_r1)*0.0005\n",
        "  loss_summed = loss_r\n",
        "  print(\"loss_summed shape:\", loss_summed.shape)\n",
        "\n",
        "  gradients_of_enc0 = enc0_tape.gradient(loss_summed, encoder0.trainable_variables)\n",
        "  gradients_of_enc1 = enc1_tape.gradient(loss_summed, encoder1.trainable_variables)\n",
        "  gradients_of_gen_start = gen_start_tape.gradient(loss_summed, generator_start.trainable_variables)\n",
        "  gradients_of_recon = recon_tape.gradient(loss_summed, reconstructor.trainable_variables)\n",
        "\n",
        "  enc0_optimizer.apply_gradients(zip(gradients_of_enc0, encoder0.trainable_variables))\n",
        "  enc1_optimizer.apply_gradients(zip(gradients_of_enc1, encoder1.trainable_variables))\n",
        "  gen_start_optimizer.apply_gradients(zip(gradients_of_gen_start, generator_start.trainable_variables))\n",
        "  recon_optimizer.apply_gradients(zip(gradients_of_recon, reconstructor.trainable_variables))\n",
        "\n",
        "  print('loss_r: ', tf.reduce_max(loss_r), tf.reduce_min(loss_r), tf.reduce_mean(loss_r))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recon0 shape: (16, 224, 224, 1)\n",
            "recon1 shape: (16, 224, 224, 1)\n",
            "images_grayscale shape: (16, 224, 224, 1)\n",
            "loss_recon0 shape: (16, 1)\n",
            "loss_summed shape: (16, 1)\n",
            "loss_r:  tf.Tensor(2.433419, shape=(), dtype=float32) tf.Tensor(1.2088357, shape=(), dtype=float32) tf.Tensor(1.649849, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJw8ugiB2Sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changes compared to the DCGAN version: test_input is the first real image(s) instead of a random seed\n",
        "def generate_and_save_images(epoch, test_input, index):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  original = test_input[index:index+1,:,:,:]\n",
        "  prediction = classifier.predict(original)\n",
        "  prediction = np.argmax(prediction, axis=1)\n",
        "  if prediction == 0:\n",
        "    z = encoder0(original, training=False)\n",
        "  if prediction == 1:\n",
        "    z = encoder1(original, training=False)\n",
        "  gen_from_pred = generator_start(z, training=False)\n",
        "  recon_from_pred = reconstructor(gen_from_pred, training=False)\n",
        "  #trafo_from_pred = transformator(gen_from_pred, training=False)\n",
        "  #mask_from_pred = mask(gen_from_pred, training=False)\n",
        "  #comp_from_pred = create_composite(original, trafo_from_pred, mask_from_pred)\n",
        "  \n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(original[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(recon_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  #plt.subplot(1, 5, 3)\n",
        "  #plt.imshow(trafo_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  #plt.axis('off')\n",
        "  #plt.subplot(1, 5, 4)\n",
        "  #plt.imshow(mask_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  #plt.axis('off')\n",
        "  #plt.subplot(1, 5, 5)\n",
        "  #plt.imshow(comp_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  #plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4tp4eItFTcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set = test_data\n",
        "#generate_and_save_images(1,set,index=20)\n",
        "#generate_and_save_images(1,set,index=21)\n",
        "#generate_and_save_images(1,set,index=22)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P8cI4NdF6V2",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCOQv8cYF89g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(images):\n",
        "  predicted_classes = classifier.predict(images) # model takes (224,224,3) as input\n",
        "  predicted_classes = np.argmax(predicted_classes, axis=1) # now we have the actual predictions\n",
        "  predicted_classes = tf.reshape(predicted_classes, [-1,1]) # make it compatible with tensors below\n",
        "  predicted_classes = tf.cast(predicted_classes, tf.float32) # change type to float for multiplications\n",
        "  \n",
        "  images_grayscale = images[:,:,:,0:1] #needed for loss_computations\n",
        "\n",
        "  with tf.GradientTape() as enc0_tape, tf.GradientTape() as enc1_tape, tf.GradientTape() as gen_start_tape, tf.GradientTape() as recon_tape:\n",
        "    z_as0 = encoder0(images)\n",
        "    z_as1 = encoder1(images)\n",
        "    gen_from_pred0 = generator_start(z_as0)\n",
        "    gen_from_pred1 = generator_start(z_as1)\n",
        "    recon_from_pred0 = reconstructor(gen_from_pred0)\n",
        "    recon_from_pred1 = reconstructor(gen_from_pred1)\n",
        "    print(\"recon0 shape:\", recon_from_pred0.shape)\n",
        "    print(\"recon1 shape:\", recon_from_pred1.shape)\n",
        "    print(\"images_grayscale shape:\", images_grayscale.shape)\n",
        "\n",
        "    loss_recon0 = loss_recon(images_grayscale, recon_from_pred0)\n",
        "    loss_recon1 = loss_recon(images_grayscale, recon_from_pred1)\n",
        "    print(\"loss_recon0 shape:\", loss_recon0.shape)\n",
        "    loss_r0 = tf.math.multiply(1-predicted_classes, loss_recon0) # set the loss for the wrong recons to 0\n",
        "    loss_r1 = tf.math.multiply(predicted_classes, loss_recon1) # set the loss for the wrong recons to 0\n",
        "    loss_r = (loss_r0 + loss_r1)*0.0005\n",
        "    loss_summed = loss_r\n",
        "    print(\"loss_summed shape:\", loss_summed.shape)\n",
        "\n",
        "    gradients_of_enc0 = enc0_tape.gradient(loss_summed, encoder0.trainable_variables)\n",
        "    gradients_of_enc1 = enc1_tape.gradient(loss_summed, encoder1.trainable_variables)\n",
        "    gradients_of_gen_start = gen_start_tape.gradient(loss_summed, generator_start.trainable_variables)\n",
        "    gradients_of_recon = recon_tape.gradient(loss_summed, reconstructor.trainable_variables)\n",
        "\n",
        "    enc0_optimizer.apply_gradients(zip(gradients_of_enc0, encoder0.trainable_variables))\n",
        "    enc1_optimizer.apply_gradients(zip(gradients_of_enc1, encoder1.trainable_variables))\n",
        "    gen_start_optimizer.apply_gradients(zip(gradients_of_gen_start, generator_start.trainable_variables))\n",
        "    recon_optimizer.apply_gradients(zip(gradients_of_recon, reconstructor.trainable_variables))\n",
        "\n",
        "    print('loss_r: ', tf.reduce_max(loss_r), tf.reduce_min(loss_r), tf.reduce_mean(loss_r))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daFoX43XG10x",
        "colab_type": "code",
        "outputId": "ef66baea-b98d-468e-c9c3-288b0b795be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "# Checking, seems to work just fine. Loss changes from step to step\n",
        "#train_step(test_data) # produces OOM error\n",
        "train_step(valid_data) # still works"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recon0 shape: (16, 224, 224, 1)\n",
            "recon1 shape: (16, 224, 224, 1)\n",
            "images_grayscale shape: (16, 224, 224, 1)\n",
            "loss_recon0 shape: (16, 1)\n",
            "loss_summed shape: (16, 1)\n",
            "loss_r:  tf.Tensor(2.4333916, shape=(), dtype=float32) tf.Tensor(1.2088183, shape=(), dtype=float32) tf.Tensor(1.6498413, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdDj6BItW9_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copied from the classifier's training. NAIN then just calls model.fit with train_data_gen as input.\n",
        "# This won't work 1 to 1 here, since ExplainGAN is not one sequential model\n",
        "# Nevertheless, I want to use the generator to avoid OOM errors and save time when training\n",
        "batch_size = 16\n",
        "nb_epochs = 10\n",
        "\n",
        "# Get a train data generator\n",
        "train_data_gen = data_gen(data=train_data, batch_size=batch_size)\n",
        "\n",
        "# Define the number of training steps\n",
        "nb_train_steps = train_data.shape[0]//batch_size\n",
        "\n",
        "print(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(valid_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI7LMTBZYbz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step(train_data_gen) # I don't know if this works. It was running over 1 hour without finishing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC00BFixFirx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # Here we use the batching of the dataset below\n",
        "      # call the function train_step defined in the box above this one\n",
        "      train_step(image_batch, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag)\n",
        "\n",
        "    # Produce images for the GIF as we go (from DCGAN)\n",
        "    #display.clear_output(wait=True)\n",
        "    generate_and_save_images(epoch + 1, train_images, index=0) # still input train_images here. Would be nice to use dataset instead!!\n",
        "    \n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)\n",
        "  generate_and_save_images(epochs, train_images, index=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwPE_rtt8o3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(18,6))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(valid_data[1, :, :, 0], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(valid_data[2, :, :, 1], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(valid_data[3, :, :, 0], cmap='gray')\n",
        "plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}