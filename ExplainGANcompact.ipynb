{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ExplainGAN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarsAmker/ExplainGAN/blob/master/ExplainGANcompact.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_jQ1tEQCxwRx"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "V_sgB_5dx1f1",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "# ExplainGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0TD5ZrvEMbhZ"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/dcgan\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/dcgan.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ITZuApL56Mny"
      },
      "source": [
        "Try to build code for ExplainGAN starting with the DCGAN tutorial from tensorflow. \n",
        "\n",
        "Original text: This tutorial demonstrates how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5oue0oqCkZZ",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WZKbyU2-AiY-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError: # If TPU not found\n",
        "  tpu = None\n",
        "\n",
        "# To generate GIFs. Let's see, maybe this can still be useful here?\n",
        "!pip install imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YfIk2es3hJEd",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avq7_9J2wrJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the github repo to get access to the outsourced building blocks\n",
        "!git clone https://github.com/LarsAmker/ExplainGAN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUNlnNov92pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run ExplainGAN/data_and_classifier\n",
        "%run ExplainGAN/encoder\n",
        "%run ExplainGAN/generators\n",
        "%run ExplainGAN/discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load and prepare the dataset\n",
        "\n",
        "Let's try the **MNIST** dataset here too. It was also one of the examples from Silberman's paper. We need the images showing **4s and 9s only**, because ExplainGAN explains binary classifier. (4,9) was one of the pairs of digits used by Silberman too (the others were (3,8) and (5,6)).\n",
        "Filter these out for the training part (60000) as well as for the test part (10000 images) of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Our classifier\n",
        "\n",
        "First we need a pre-trained **binary** classifier. Modify the one from the tensorflow tutorial \"Basic Image Classification\" on Fashion-MNIST classification.\n",
        "\n",
        "This classifier is the AI that we aim to explain with ExplainGAN. It should not interact a lot with the ExplainGAN part (except for being used by it at the end of the process) and thus be fairly interchangeable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyPs77Qixr2h",
        "colab_type": "text"
      },
      "source": [
        "## The actual ExplainGAN part\n",
        "\n",
        "On top of that, there are two encoders (not present in DCGAN), 3 generators (Sharing the first few layers) that produce three images (reconstruction, transformation and mask) and two discriminators used for training this generator. My interpretation of Silberman's very sparse paragraph about the ExplainGAN model architecture is that the encoders and discriminators are similar to the DCGAN discriminator and the generator is similar to DCGAN's generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TivYEDyhEUsL",
        "colab_type": "text"
      },
      "source": [
        "### The encoders\n",
        "\n",
        "There are two encoders, one for each predicted class. They take an image and produce a compressed, encoded so-called latent variable z that the generator uses as input. In the tf tutorial for DCGAN on MNIST, the generator's input was an array of 100 standard normal r.v.s. Let's go for a latent variable of size 128 here (in Silberman's ExplainGAN paper, there is no information about the dimension of the encoded array). I also looked at the Variational Auto Encoder tf tutorial, but it is kind of complicated, so I went for the source below instead (which is also using MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q65nZbN1Rn3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder4 = make_encoder_model(activ_fct='relu')\n",
        "encoder9 = make_encoder_model(activ_fct='relu')\n",
        "#encoder4.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### The generator and the mask function\n",
        "\n",
        "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from the latent variable z produced from one of the encoders. Start with a `Dense` layer that takes z as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh.\n",
        "\n",
        "Only change made compared to the DCGAN architecture: Input size is 128 now for compatibility with the encoders. As an alternaive to the DCGAN generator, we could use the decoder from the autoencoder source (used for the encoders above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_89viFKN42cK",
        "colab": {}
      },
      "source": [
        "generator_start = make_generator_model_start()\n",
        "#generator_start.summary()\n",
        "# Batch normalization has 4 parameters per channel. Two of them are trainable (gamma and beta mentioned in Szegedy)\n",
        "# The other 2 are not trainable (maybe epsilon and momentum?). I found this out by experimentation with a toy model.\n",
        "\n",
        "reconstructor = make_generator_model_end_tanh()\n",
        "transformator = make_generator_model_end_tanh()\n",
        "mask = make_generator_model_end_sigmoid()\n",
        "#reconstructor.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Use the (as yet untrained) generators to create some example images for tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEladWfPIU0W",
        "colab_type": "text"
      },
      "source": [
        "### Create composite images\n",
        "\n",
        "Now that we have a reconstruction, transformation and the mask which will be trained to show the differences, we still need to combine these results with the original input images to get the composite images - our final product that we feed into the discriminators (along with the recon and trafo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSTNdIXwJDOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will need to call this function twice to create a composite image of each class\n",
        "# orig_image and created_image are always from the two different classes. \n",
        "# tf.math.multiply performs an element-wise multiplication\n",
        "def create_composite(orig_image, created_image, mask):\n",
        "  composite = tf.add(tf.math.multiply(1-mask,orig_image), tf.math.multiply((mask),created_image))\n",
        "  return composite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The discriminators\n",
        "\n",
        "The discriminator is a CNN-based image classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gDkA05NE6QMs",
        "colab": {}
      },
      "source": [
        "discriminator4 = make_discriminator_model()\n",
        "discriminator9 = make_discriminator_model()\n",
        "#decision = discriminator9(trafo_image)\n",
        "#print (decision)\n",
        "#discriminator0.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss and optimizers\n",
        "\n",
        "Define all loss functions and optimizers needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V5sj7LDPU-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run ExplainGAN/losses/loss_gan\n",
        "%run ExplainGAN/losses/loss_classifier\n",
        "%run ExplainGAN/losses/loss_reconstruction\n",
        "%run ExplainGAN/losses/losses_prior"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MgIc7i0th_Iu"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "DCGAN: Use one optimizer for discriminator and one for the generator\n",
        "\n",
        "How about ExplainGAN? We have a bunch of networks. What will be trained separately and what not? Let's try one optimizer for each network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "source": [
        "# Add optimizer for all the different parts: enc4, enc9, gen_start, recon, trafo, disc4, disc9\n",
        "enc4_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "enc9_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "gen_start_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "recon_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "trafo_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "mask_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc4_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc9_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "# I could change the Adam parameter. The CDGAN paper suggested 2e-4 instead of 1e-4. It also suggested to change another parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Define the training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jylSonrqSWfi"
      },
      "source": [
        "Go through the whole architecture in one training step. As input use a selection of real images, it is not important how many of each class we take. At the start of the process, the classifier puts predicted labels on each image. Then, all pictures go through the other parts of the network twice, once for each possible predicted class. When computing the losses at the end, only use the actual predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVCOtgRlJNkr",
        "colab": {}
      },
      "source": [
        "# From DCGAN original: Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "#@tf.function # produces an error in my code, if I leave it away there is no problem\n",
        "def train_step(images, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, pretraining_flag): \n",
        "    # images is the whole batch of real images fed into the machine in one training step\n",
        "    # pretraining_flag = 1: Only train the reconstruction part of the network. 0: Don't train encoders anymore\n",
        "    predicted_classes = classifier.predict(images)\n",
        "    predicted_classes = np.argmax(predicted_classes, axis=1) # now we have the actual predictions\n",
        "    predicted_classes = tf.reshape(predicted_classes, [-1,1]) # make it compatible with tensors below\n",
        "    predicted_classes = tf.cast(predicted_classes, tf.float32) # change type to float for multiplications\n",
        "    \n",
        "    # More components (8 to be precise) in this network than in DCGAN -> Needs more gradient tapes\n",
        "    # List: enc4_tape, enc9_tape, gen_start_tape, recon_tape, trafo_tape, disc4_tape, disc9_tape\n",
        "    with tf.GradientTape() as enc4_tape, tf.GradientTape() as enc9_tape, tf.GradientTape() as gen_start_tape, tf.GradientTape() as recon_tape, tf.GradientTape() as trafo_tape, tf.GradientTape() as mask_tape, tf.GradientTape() as disc4_tape, tf.GradientTape() as disc9_tape:\n",
        "      # Put all images through _both_ streams. Only use one of them for each image for loss computation\n",
        "      z_as4 = encoder4(images)\n",
        "      z_as9 = encoder9(images)\n",
        "      # 'middle step', apply the part of the generator shared by trafo, recon and mask\n",
        "      gen_from_pred4 = generator_start(z_as4)\n",
        "      gen_from_pred9 = generator_start(z_as9)\n",
        "      # reconstructions, transformations and masks, then create composites\n",
        "      recon_from_pred4 = reconstructor(gen_from_pred4)\n",
        "      recon_from_pred9 = reconstructor(gen_from_pred9)\n",
        "      trafo_from_pred4 = transformator(gen_from_pred4)\n",
        "      trafo_from_pred9 = transformator(gen_from_pred9)\n",
        "      mask_from_pred4 = mask(gen_from_pred4)\n",
        "      mask_from_pred9 = mask(gen_from_pred9) \n",
        "      comp_from_pred4 = create_composite(images, trafo_from_pred4, mask_from_pred4)\n",
        "      comp_from_pred9 = create_composite(images, trafo_from_pred9, mask_from_pred9)\n",
        "\n",
        "      # now we need to get the losses right\n",
        "      # GAN loss - first create all necessary discriminator outputs\n",
        "      real_output4 = discriminator4(images)\n",
        "      real_output9 = discriminator9(images)\n",
        "      recon_output4 = discriminator4(recon_from_pred4)\n",
        "      recon_output9 = discriminator9(recon_from_pred9)\n",
        "      trafo_output4 = discriminator9(trafo_from_pred4) # now use the opposite discriminators\n",
        "      trafo_output9 = discriminator4(trafo_from_pred9)\n",
        "      comp_output4 = discriminator9(comp_from_pred4)\n",
        "      comp_output9 = discriminator4(comp_from_pred9)\n",
        "      # now calculate the loss (split up by predicted class, not by produced class as in Silberman's paper)\n",
        "      \n",
        "      loss_gan_pred4 = loss_gan(real_output4, recon_output4, trafo_output4, comp_output4)\n",
        "      loss_gan_pred9 = loss_gan(real_output9, recon_output9, trafo_output9, comp_output9)\n",
        "      loss_g4 = tf.math.multiply(1-predicted_classes, loss_gan_pred4)\n",
        "      loss_g9 = tf.math.multiply(predicted_classes, loss_gan_pred9)\n",
        "      loss_g = (loss_g4 + loss_g9) * weight_g\n",
        "      \n",
        "      # classifier loss - first we need to create predictions for our composite images\n",
        "      pred_comp_from_pred4 = classifier(comp_from_pred4)\n",
        "      pred_comp_from_pred4 = pred_comp_from_pred4[:,1] # not argmax here, we need the probability of a 9\n",
        "      pred_comp_from_pred4 = tf.reshape(pred_comp_from_pred4, [-1,1])\n",
        "      pred_comp_from_pred4 = tf.cast(pred_comp_from_pred4, tf.float32)\n",
        "      pred_comp_from_pred9 = classifier(comp_from_pred9)\n",
        "      pred_comp_from_pred9 = pred_comp_from_pred9[:,1] # not argmax here, we need the probability\n",
        "      pred_comp_from_pred9 = tf.reshape(pred_comp_from_pred9, [-1,1])\n",
        "      pred_comp_from_pred9 = tf.cast(pred_comp_from_pred9, tf.float32)\n",
        "      # now calculate the loss\n",
        "      loss_class_pred4 = loss_classifier4(pred_comp_from_pred4) # put a composite 9 into the loss_c for predictions 4\n",
        "      loss_class_pred9 = loss_classifier9(pred_comp_from_pred9)\n",
        "      loss_c4 = tf.math.multiply(1-predicted_classes, loss_class_pred4)\n",
        "      loss_c9 = tf.math.multiply(predicted_classes, loss_class_pred9)\n",
        "      loss_c = (loss_c4 + loss_c9) * weight_c\n",
        "      \n",
        "      # reconstruction loss loss_r\n",
        "      loss_recon4 = loss_recon(images, recon_from_pred4)\n",
        "      loss_recon9 = loss_recon(images, recon_from_pred9)\n",
        "      loss_r4 = tf.math.multiply(1-predicted_classes, loss_recon4) # set the loss for the wrong recons to 0\n",
        "      loss_r9 = tf.math.multiply(predicted_classes, loss_recon9) # set the loss for the wrong recons to 0\n",
        "      loss_r = (loss_r4 + loss_r9) * weight_r\n",
        "\n",
        "      # Add up losses that are used together\n",
        "      loss_summed = loss_g + loss_c + loss_r\n",
        "\n",
        "      if pretraining_flag == 1:\n",
        "        gradients_of_enc4 = enc4_tape.gradient(loss_summed, encoder4.trainable_variables)\n",
        "        gradients_of_enc9 = enc9_tape.gradient(loss_summed, encoder9.trainable_variables)\n",
        "        enc4_optimizer.apply_gradients(zip(gradients_of_enc4, encoder4.trainable_variables))\n",
        "        enc9_optimizer.apply_gradients(zip(gradients_of_enc9, encoder9.trainable_variables))\n",
        "      \n",
        "      gradients_of_gen_start = gen_start_tape.gradient(loss_summed, generator_start.trainable_variables)\n",
        "      gradients_of_recon = recon_tape.gradient(loss_summed, reconstructor.trainable_variables)\n",
        "      gen_start_optimizer.apply_gradients(zip(gradients_of_gen_start, generator_start.trainable_variables))\n",
        "      recon_optimizer.apply_gradients(zip(gradients_of_recon, reconstructor.trainable_variables))\n",
        "\n",
        "      print('###################################################################')\n",
        "      print('real_output4: ', tf.reduce_max(real_output4), tf.reduce_min(real_output4))\n",
        "      print('recon_output4: ', tf.reduce_max(recon_output4), tf.reduce_min(recon_output4))\n",
        "      print('trafo_output4: ', tf.reduce_max(trafo_output4), tf.reduce_min(trafo_output4))\n",
        "      print('comp_output4: ', tf.reduce_max(comp_output4), tf.reduce_min(comp_output4), tf.reduce_mean(comp_output4))\n",
        "      print('loss_g: ', tf.reduce_max(loss_g), tf.reduce_min(loss_g), tf.reduce_mean(loss_g))\n",
        "      print('loss_c: ', tf.reduce_max(loss_c), tf.reduce_min(loss_c), tf.reduce_mean(loss_c))\n",
        "      print('loss_r: ', tf.reduce_max(loss_r), tf.reduce_min(loss_r), tf.reduce_mean(loss_r))\n",
        "  \n",
        "\n",
        "      if pretraining_flag == 0:\n",
        "        # the 4 prior losses, try kappa=0.03 in loss_count\n",
        "        kappa = 0.03\n",
        "        loss_const4 = loss_const(images, trafo_from_pred4, mask_from_pred4)\n",
        "        loss_const9 = loss_const(images, trafo_from_pred9, mask_from_pred9)\n",
        "        loss_cs4 = tf.math.multiply(1-predicted_classes, loss_const4)\n",
        "        loss_cs9 = tf.math.multiply(predicted_classes, loss_const9)\n",
        "        loss_cs = (loss_cs4 + loss_cs9) * weight_cs\n",
        "        loss_count4 = loss_count(mask_from_pred4, kappa)\n",
        "        loss_count9 = loss_count(mask_from_pred9, kappa)\n",
        "        loss_ct4 = tf.math.multiply(1-predicted_classes, loss_count4)\n",
        "        loss_ct9 = tf.math.multiply(predicted_classes, loss_count9)\n",
        "        loss_ct = (loss_ct4 + loss_ct9) * weight_ct\n",
        "        loss_smooth4 = loss_smoothness(mask_from_pred4)\n",
        "        loss_smooth9 = loss_smoothness(mask_from_pred9)\n",
        "        loss_sm4 = tf.math.multiply(1-predicted_classes, loss_smooth4)\n",
        "        loss_sm9 = tf.math.multiply(predicted_classes, loss_smooth9)\n",
        "        loss_sm = (loss_sm4 + loss_sm9) * weight_sm\n",
        "        loss_entropy4 = loss_entropy(mask_from_pred4)\n",
        "        loss_entropy9 = loss_entropy(mask_from_pred9)\n",
        "        loss_en4 = tf.math.multiply(1-predicted_classes, loss_entropy4)\n",
        "        loss_en9 = tf.math.multiply(predicted_classes, loss_entropy9)\n",
        "        loss_en = (loss_en4 + loss_en9) * weight_en\n",
        "        loss_prior = loss_cs + loss_ct + loss_sm + loss_en\n",
        "\n",
        "        gradients_of_trafo = trafo_tape.gradient(loss_g + loss_c, transformator.trainable_variables)\n",
        "        gradients_of_mask = mask_tape.gradient(loss_prior + loss_g + loss_c, mask.trainable_variables)\n",
        "        gradients_of_disc4 = disc4_tape.gradient(-loss_g, discriminator4.trainable_variables)\n",
        "        gradients_of_disc9 = disc9_tape.gradient(-loss_g, discriminator9.trainable_variables)\n",
        "\n",
        "        trafo_optimizer.apply_gradients(zip(gradients_of_trafo, transformator.trainable_variables))\n",
        "        mask_optimizer.apply_gradients(zip(gradients_of_mask, mask.trainable_variables))\n",
        "        disc4_optimizer.apply_gradients(zip(gradients_of_disc4, discriminator4.trainable_variables))      \n",
        "        disc9_optimizer.apply_gradients(zip(gradients_of_disc9, discriminator9.trainable_variables))\n",
        "\n",
        "        print('loss_cs: ', tf.reduce_max(loss_cs), tf.reduce_min(loss_cs), tf.reduce_mean(loss_cs))\n",
        "        print('loss_ct: ', tf.reduce_max(loss_ct), tf.reduce_min(loss_ct), tf.reduce_mean(loss_ct))\n",
        "        print('loss_sm: ', tf.reduce_max(loss_sm), tf.reduce_min(loss_sm), tf.reduce_mean(loss_sm))\n",
        "        print('loss_en: ', tf.reduce_max(loss_en), tf.reduce_min(loss_en), tf.reduce_mean(loss_en))\n",
        "\n",
        "\n",
        "      #print('z_as4: ', tf.reduce_max(z_as4), tf.reduce_min(z_as4))\n",
        "      #print('gen_from_pred4: ', tf.reduce_max(gen_from_pred4), tf.reduce_min(gen_from_pred4))\n",
        "      #print('recon_from_pred4: ', tf.reduce_max(recon_from_pred4), tf.reduce_min(recon_from_pred4))\n",
        "      #print('comp_output9: ', tf.reduce_max(comp_output9), tf.reduce_min(comp_output9), tf.reduce_mean(comp_output9))\n",
        "      #print('loss_summed: ', tf.reduce_max(loss_summed), tf.reduce_min(loss_summed))\n",
        "      #print(z_as4.shape, real_output4.shape, loss_g.shape, loss_summed.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2M7LmLtGEMQJ",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, pretraining_flag):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # Here we use the batching of the dataset below\n",
        "      # call the function train_step defined in the box above this one\n",
        "      train_step(image_batch, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, pretraining_flag)\n",
        "\n",
        "    # Produce images for the GIF as we go (from DCGAN)\n",
        "    #display.clear_output(wait=True)\n",
        "    generate_and_save_images(epoch + 1, train_images, index=0) # still input train_images here. Would be nice to use dataset instead!!\n",
        "    \n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)\n",
        "  generate_and_save_images(epochs, train_images, index=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2aFF7Hk3XdeW"
      },
      "source": [
        "**Generate and save images**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RmdVsmvhPxyy",
        "colab": {}
      },
      "source": [
        "# Changes compared to the DCGAN version: test_input is the first real image(s) instead of a random seed\n",
        "def generate_and_save_images(epoch, test_input, index):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  original = test_input[index:index+1,:,:,:]\n",
        "  prediction = classifier.predict(original)\n",
        "  prediction = np.argmax(prediction, axis=1)\n",
        "  if prediction == 0:\n",
        "    z = encoder4(original, training=False)\n",
        "  if prediction == 1:\n",
        "    z = encoder9(original, training=False)\n",
        "  gen_from_pred = generator_start(z, training=False)\n",
        "  recon_from_pred = reconstructor(gen_from_pred, training=False)\n",
        "  trafo_from_pred = transformator(gen_from_pred, training=False)\n",
        "  mask_from_pred = mask(gen_from_pred, training=False)\n",
        "  comp_from_pred = create_composite(original, trafo_from_pred, mask_from_pred)\n",
        "  \n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  plt.subplot(1, 5, 1)\n",
        "  plt.imshow(original[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 2)\n",
        "  plt.imshow(recon_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 3)\n",
        "  plt.imshow(trafo_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 4)\n",
        "  plt.imshow(mask_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 5)\n",
        "  plt.imshow(comp_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gsckd2SBhAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_and_save_images(1,train_images,index=20)\n",
        "generate_and_save_images(1,train_images,index=21)\n",
        "generate_and_save_images(1,train_images,index=22)\n",
        "generate_and_save_images(1,train_images,index=23)\n",
        "generate_and_save_images(1,train_images,index=24)\n",
        "generate_and_save_images(1,train_images,index=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "## Train the model\n",
        "Call the `train()` method defined above to train the generators and discriminator and the other networks simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
        "\n",
        "DCGAN: At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDdf2OfwxkUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch and shuffle the data in DCGAN - do I need to shuffle?\n",
        "# We can play around with the batch size a bit. Smaller batch sizes make computations faster\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(len(train_labels)).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(len(test_labels)).batch(BATCH_SIZE)\n",
        "EPOCHS = 100\n",
        "num_examples_to_generate = 4 # the gif is not super important. Ignore it for now"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ly3UN0SLLY2l",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# loss weights: Scale everything to be close to 3 (leave the only negative loss, GAN loss, which is around -2.8 untouched)\n",
        "# I might need to change a lot here. No idea about the weights used by Silberman\n",
        "weight_g = 0\n",
        "weight_c = 0\n",
        "weight_r = 1\n",
        "weight_cs = 0\n",
        "weight_ct = 0\n",
        "weight_sm = 0\n",
        "weight_en = 0\n",
        "pretraining_flag = 1\n",
        "train(test_dataset, EPOCHS, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, pretraining_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0DiUfmmFFCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#weight_g = 1\n",
        "#weight_c = 0.33\n",
        "#weight_r = 0.004\n",
        "#weight_cs = 0.004\n",
        "#weight_ct = 300\n",
        "#weight_sm = 0.15\n",
        "#weight_en = 8.5\n",
        "weight_g = 1\n",
        "weight_c = 1\n",
        "weight_r = 1\n",
        "weight_cs = 1\n",
        "weight_ct = 1\n",
        "weight_sm = 1\n",
        "weight_en = 1\n",
        "pretraining_flag = 0\n",
        "train(test_dataset, 1, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, pretraining_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P4M_vIbUi7c0"
      },
      "source": [
        "## Create a GIF (ignored for now)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WfO5wCdclHGL",
        "colab": {}
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "#def display_image(epoch_no):\n",
        "#  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5x3q9_Oe5q0A",
        "colab": {}
      },
      "source": [
        "#display_image(EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NywiH3nL8guF"
      },
      "source": [
        "Use `imageio` to create an animated gif using the images saved during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IGKQgENQ8lEI",
        "colab": {}
      },
      "source": [
        "#anim_file = 'dcgan.gif'\n",
        "\n",
        "#with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "#  filenames = glob.glob('image*.png')\n",
        "#  filenames = sorted(filenames)\n",
        "#  last = -1\n",
        "#  for i,filename in enumerate(filenames):\n",
        "#    frame = 2*(i**0.5)\n",
        "#    if round(frame) > round(last):\n",
        "#      last = frame\n",
        "#    else:\n",
        "#      continue\n",
        "#    image = imageio.imread(filename)\n",
        "#    writer.append_data(image)\n",
        "#  image = imageio.imread(filename)\n",
        "#  writer.append_data(image)\n",
        "\n",
        "#import IPython\n",
        "#if IPython.version_info > (6,2,0,''):\n",
        "#  display.Image(filename=anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGhC3-fMWSwl"
      },
      "source": [
        "If you're working in Colab you can download the animation with the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uV0yiKpzNP1b",
        "colab": {}
      },
      "source": [
        "#try:\n",
        "#  from google.colab import files\n",
        "#except ImportError:\n",
        "#   pass\n",
        "#else:\n",
        "#  files.download(anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k6qC-SbjK0yW"
      },
      "source": [
        "## Next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xjjkT9KAK6H7"
      },
      "source": [
        "This tutorial has shown the complete code necessary to write and train a GAN. As a next step, you might like to experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset/home). To learn more about GANs we recommend the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n"
      ]
    }
  ]
}