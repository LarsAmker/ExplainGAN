{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbZ3C2VNQjVw2nNjctEZ++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LarsAmker/ExplainGAN/blob/master/Pneumonia_compact_workinprogress_0502.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66-W42Llgq3E",
        "colab_type": "text"
      },
      "source": [
        "Mount my google drive to get access to data files and weights stored there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDU7J7Czx-7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "141afa26-66d0-442d-cba8-316cad7c7c38"
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD-lNB19g0PM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "754d23f7-135e-4168-e860-03976829c212"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "import shutil\n",
        "import imgaug as aug\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import imgaug.augmenters as iaa\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "color = sns.color_palette()\n",
        "%matplotlib inline\n",
        "\n",
        "# added tensorflow and stuff from the MNIST classifier:\n",
        "import tensorflow as tf\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import PIL\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvEF_xzy7mL1",
        "colab_type": "text"
      },
      "source": [
        "NAIN had a code box fixing some random seeds here. Not compatible with tf2 however, therefore deleted and saved in testPneumonia/reproducability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnsw07S7_mCe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "4ae33c15-38f0-4cd3-d35b-e29f235f0c47"
      },
      "source": [
        "# Clone the github repo to get access to the outsourced building blocks\n",
        "!git clone https://github.com/LarsAmker/ExplainGAN"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ExplainGAN'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/37)\u001b[K\rremote: Counting objects:   5% (2/37)\u001b[K\rremote: Counting objects:   8% (3/37)\u001b[K\rremote: Counting objects:  10% (4/37)\u001b[K\rremote: Counting objects:  13% (5/37)\u001b[K\rremote: Counting objects:  16% (6/37)\u001b[K\rremote: Counting objects:  18% (7/37)\u001b[K\rremote: Counting objects:  21% (8/37)\u001b[K\rremote: Counting objects:  24% (9/37)\u001b[K\rremote: Counting objects:  27% (10/37)\u001b[K\rremote: Counting objects:  29% (11/37)\u001b[K\rremote: Counting objects:  32% (12/37)\u001b[K\rremote: Counting objects:  35% (13/37)\u001b[K\rremote: Counting objects:  37% (14/37)\u001b[K\rremote: Counting objects:  40% (15/37)\u001b[K\rremote: Counting objects:  43% (16/37)\u001b[K\rremote: Counting objects:  45% (17/37)\u001b[K\rremote: Counting objects:  48% (18/37)\u001b[K\rremote: Counting objects:  51% (19/37)\u001b[K\rremote: Counting objects:  54% (20/37)\u001b[K\rremote: Counting objects:  56% (21/37)\u001b[K\rremote: Counting objects:  59% (22/37)\u001b[K\rremote: Counting objects:  62% (23/37)\u001b[K\rremote: Counting objects:  64% (24/37)\u001b[K\rremote: Counting objects:  67% (25/37)\u001b[K\rremote: Counting objects:  70% (26/37)\u001b[K\rremote: Counting objects:  72% (27/37)\u001b[K\rremote: Counting objects:  75% (28/37)\u001b[K\rremote: Counting objects:  78% (29/37)\u001b[K\rremote: Counting objects:  81% (30/37)\u001b[K\rremote: Counting objects:  83% (31/37)\u001b[K\rremote: Counting objects:  86% (32/37)\u001b[K\rremote: Counting objects:  89% (33/37)\u001b[K\rremote: Counting objects:  91% (34/37)\u001b[K\rremote: Counting objects:  94% (35/37)\u001b[K\rremote: Counting objects:  97% (36/37)\u001b[K\rremote: Counting objects: 100% (37/37)\u001b[K\rremote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/37)\u001b[K\rremote: Compressing objects:   5% (2/37)\u001b[K\rremote: Compressing objects:   8% (3/37)\u001b[K\rremote: Compressing objects:  10% (4/37)\u001b[K\rremote: Compressing objects:  13% (5/37)\u001b[K\rremote: Compressing objects:  16% (6/37)\u001b[K\rremote: Compressing objects:  18% (7/37)\u001b[K\rremote: Compressing objects:  21% (8/37)\u001b[K\rremote: Compressing objects:  24% (9/37)\u001b[K\rremote: Compressing objects:  27% (10/37)\u001b[K\rremote: Compressing objects:  29% (11/37)\u001b[K\rremote: Compressing objects:  32% (12/37)\u001b[K\rremote: Compressing objects:  35% (13/37)\u001b[K\rremote: Compressing objects:  37% (14/37)\u001b[K\rremote: Compressing objects:  40% (15/37)\u001b[K\rremote: Compressing objects:  43% (16/37)\u001b[K\rremote: Compressing objects:  45% (17/37)\u001b[K\rremote: Compressing objects:  48% (18/37)\u001b[K\rremote: Compressing objects:  51% (19/37)\u001b[K\rremote: Compressing objects:  54% (20/37)\u001b[K\rremote: Compressing objects:  56% (21/37)\u001b[K\rremote: Compressing objects:  59% (22/37)\u001b[K\rremote: Compressing objects:  62% (23/37)\u001b[K\rremote: Compressing objects:  64% (24/37)\u001b[K\rremote: Compressing objects:  67% (25/37)\u001b[K\rremote: Compressing objects:  70% (26/37)\u001b[K\rremote: Compressing objects:  72% (27/37)\u001b[K\rremote: Compressing objects:  75% (28/37)\u001b[K\rremote: Compressing objects:  78% (29/37)\u001b[K\rremote: Compressing objects:  81% (30/37)\u001b[K\rremote: Compressing objects:  83% (31/37)\u001b[K\rremote: Compressing objects:  86% (32/37)\u001b[K\rremote: Compressing objects:  89% (33/37)\u001b[K\rremote: Compressing objects:  91% (34/37)\u001b[K\rremote: Compressing objects:  94% (35/37)\u001b[K\rremote: Compressing objects:  97% (36/37)\u001b[K\rremote: Compressing objects: 100% (37/37)\u001b[K\rremote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "Receiving objects:   0% (1/640)   \rReceiving objects:   1% (7/640)   \rReceiving objects:   2% (13/640)   \rReceiving objects:   3% (20/640)   \rReceiving objects:   4% (26/640)   \rReceiving objects:   5% (32/640)   \rReceiving objects:   6% (39/640)   \rReceiving objects:   7% (45/640)   \rReceiving objects:   8% (52/640)   \rReceiving objects:   9% (58/640)   \rReceiving objects:  10% (64/640)   \rReceiving objects:  11% (71/640)   \rReceiving objects:  12% (77/640)   \rReceiving objects:  13% (84/640)   \rReceiving objects:  14% (90/640)   \rReceiving objects:  15% (96/640)   \rReceiving objects:  16% (103/640)   \rReceiving objects:  17% (109/640)   \rReceiving objects:  18% (116/640)   \rReceiving objects:  19% (122/640)   \rReceiving objects:  20% (128/640)   \rReceiving objects:  21% (135/640)   \rReceiving objects:  22% (141/640)   \rReceiving objects:  23% (148/640)   \rReceiving objects:  24% (154/640)   \rReceiving objects:  25% (160/640)   \rReceiving objects:  26% (167/640)   \rReceiving objects:  27% (173/640)   \rReceiving objects:  28% (180/640)   \rReceiving objects:  29% (186/640)   \rReceiving objects:  30% (192/640)   \rReceiving objects:  31% (199/640)   \rReceiving objects:  32% (205/640)   \rReceiving objects:  33% (212/640)   \rReceiving objects:  34% (218/640)   \rReceiving objects:  35% (224/640)   \rReceiving objects:  36% (231/640)   \rReceiving objects:  37% (237/640)   \rReceiving objects:  38% (244/640)   \rReceiving objects:  39% (250/640)   \rReceiving objects:  40% (256/640)   \rReceiving objects:  41% (263/640)   \rReceiving objects:  42% (269/640)   \rReceiving objects:  43% (276/640)   \rReceiving objects:  44% (282/640)   \rReceiving objects:  45% (288/640)   \rReceiving objects:  46% (295/640)   \rReceiving objects:  47% (301/640)   \rReceiving objects:  48% (308/640)   \rReceiving objects:  49% (314/640)   \rReceiving objects:  50% (320/640)   \rReceiving objects:  51% (327/640)   \rReceiving objects:  52% (333/640)   \rReceiving objects:  53% (340/640)   \rReceiving objects:  54% (346/640)   \rReceiving objects:  55% (352/640)   \rReceiving objects:  56% (359/640)   \rReceiving objects:  57% (365/640)   \rReceiving objects:  58% (372/640)   \rReceiving objects:  59% (378/640)   \rReceiving objects:  60% (384/640)   \rReceiving objects:  61% (391/640)   \rReceiving objects:  62% (397/640)   \rReceiving objects:  63% (404/640)   \rReceiving objects:  64% (410/640)   \rReceiving objects:  65% (416/640)   \rReceiving objects:  66% (423/640)   \rReceiving objects:  67% (429/640)   \rReceiving objects:  68% (436/640)   \rReceiving objects:  69% (442/640)   \rReceiving objects:  70% (448/640)   \rReceiving objects:  71% (455/640)   \rReceiving objects:  72% (461/640)   \rReceiving objects:  73% (468/640)   \rReceiving objects:  74% (474/640)   \rReceiving objects:  75% (480/640)   \rReceiving objects:  76% (487/640)   \rReceiving objects:  77% (493/640)   \rReceiving objects:  78% (500/640)   \rReceiving objects:  79% (506/640)   \rReceiving objects:  80% (512/640)   \rReceiving objects:  81% (519/640)   \rReceiving objects:  82% (525/640)   \rReceiving objects:  83% (532/640)   \rReceiving objects:  84% (538/640)   \rReceiving objects:  85% (544/640)   \rReceiving objects:  86% (551/640)   \rReceiving objects:  87% (557/640)   \rReceiving objects:  88% (564/640)   \rReceiving objects:  89% (570/640)   \rReceiving objects:  90% (576/640)   \rReceiving objects:  91% (583/640)   \rReceiving objects:  92% (589/640)   \rReceiving objects:  93% (596/640)   \rReceiving objects:  94% (602/640)   \rReceiving objects:  95% (608/640)   \rReceiving objects:  96% (615/640)   \rremote: Total 640 (delta 22), reused 0 (delta 0), pack-reused 603\u001b[K\n",
            "Receiving objects:  97% (621/640)   \rReceiving objects:  98% (628/640)   \rReceiving objects:  99% (634/640)   \rReceiving objects: 100% (640/640)   \rReceiving objects: 100% (640/640), 2.26 MiB | 12.77 MiB/s, done.\n",
            "Resolving deltas:   0% (0/373)   \rResolving deltas:   1% (4/373)   \rResolving deltas:  14% (53/373)   \rResolving deltas:  15% (58/373)   \rResolving deltas:  16% (61/373)   \rResolving deltas:  17% (66/373)   \rResolving deltas:  20% (78/373)   \rResolving deltas:  21% (80/373)   \rResolving deltas:  22% (83/373)   \rResolving deltas:  23% (87/373)   \rResolving deltas:  27% (103/373)   \rResolving deltas:  28% (107/373)   \rResolving deltas:  31% (117/373)   \rResolving deltas:  32% (120/373)   \rResolving deltas:  33% (125/373)   \rResolving deltas:  54% (204/373)   \rResolving deltas:  63% (236/373)   \rResolving deltas:  79% (297/373)   \rResolving deltas:  93% (349/373)   \rResolving deltas:  96% (361/373)   \rResolving deltas:  99% (371/373)   \rResolving deltas: 100% (373/373)   \rResolving deltas: 100% (373/373), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXUkM73Hh5Wz",
        "colab_type": "text"
      },
      "source": [
        "## Load the preprocessed data and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWxqKQcifXDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.load('/content/gdrive/My Drive/pneumonia_data/train_data.npy')\n",
        "test_data = np.load('/content/gdrive/My Drive/pneumonia_data/test_data.npy')\n",
        "valid_data = np.load('/content/gdrive/My Drive/pneumonia_data/valid_data.npy')\n",
        "train_labels = np.load('/content/gdrive/My Drive/pneumonia_data/train_labels.npy')\n",
        "test_labels = np.load('/content/gdrive/My Drive/pneumonia_data/test_labels.npy')\n",
        "valid_labels = np.load('/content/gdrive/My Drive/pneumonia_data/valid_labels.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_weBbkBOftM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(valid_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "print(valid_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi2uV41X7qVs",
        "colab_type": "text"
      },
      "source": [
        "The dataset is divided into three sets: 1) train set 2) validation set and 3) test set. Let's grab the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxdElOCu6F-",
        "colab_type": "text"
      },
      "source": [
        "In NAIN's classifier, the training data is handled differently: He just created a dataframe of paths to the images and the labels in a second column. Then he used a data generator for the training. Using the generator in the training of my multi-network ExplainGAN model did not work (or at least I did not figure out how to make it work), so I transfered the data import steps of validation and test data and I do the batching in the way of my MNIST code later on, before the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLX5Kddl-zhj",
        "colab_type": "text"
      },
      "source": [
        "# Classifier (NAIN's depthwise kernel)\n",
        "\n",
        "Build and compile an empty model and then load the weights for it from my google drive. These weights were created and saved in the notebook \"PneumoniaClassifier.ipynb\". Later: Try to get the results a bit better and achieve the same quality as NAIN did.\n",
        "\n",
        "And try NAIN's other kernel (training with focal loss). Its false predictions are more balanced between the 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09V8qpGdjbKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run ExplainGAN/blocksPneumonia/classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4K3k3vn_Roz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier =  build_model()\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI5o5UGccsam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "opt = tf.keras.optimizers.Adam(lr=0.0001, decay=1e-5)\n",
        "es = EarlyStopping(patience=5)\n",
        "chkpt = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)\n",
        "classifier.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GsrqKL3qxNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.load_weights('/content/gdrive/My Drive/Colab Notebooks/DepthwiseWeights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP9zfa7QwK9l",
        "colab_type": "text"
      },
      "source": [
        "### Testing classifier\n",
        "More visualizations like the confusion matrix can be found in testsPneumonia/test_classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waOSOfxifEIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation on test dataset\n",
        "test_loss, test_score = classifier.evaluate(test_data, test_labels, batch_size=16)\n",
        "print(\"Loss on test set: \", test_loss)\n",
        "print(\"Accuracy on test set: \", test_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgKszYAttUBJ",
        "colab_type": "text"
      },
      "source": [
        "# ExplainGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwiRAo4WvfUu",
        "colab_type": "text"
      },
      "source": [
        "## Encoder\n",
        "Maybe change something about the structure here: The Pix2Pix tutorial from TensorFlow (producing great crisp artificial images from similarly sized images) does not use MaxPooling, but accomplishes the dimensionality reduction with stride 2 convolutions. Right now, my convolutions keep the image size the same and the MaxPooling does the dim reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rP-YGr4viAyP",
        "colab": {}
      },
      "source": [
        "#%run ExplainGAN/blocksPneumonia/encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDKAK2bIxxz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copied from https://blog.keras.io/building-autoencoders-in-keras.html, added flattening at the end\n",
        "# Also made activation function variable. Relu has been leading to exploding latent variables at some point\n",
        "\n",
        "# In comparison to MNIST: Change input shape, but use only one of the three RGB channels\n",
        "# 20200419: Added one iteration and increased number of filters\n",
        "def make_encoder_model(activ_fct):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=2, activation=activ_fct, padding='same', use_bias=True,\n",
        "                                     input_shape=[224, 224, 1]))\n",
        "    #model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), strides=2, activation=activ_fct, padding='same', use_bias=True))\n",
        "    #model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (3, 3), strides=2, activation=activ_fct, padding='same', use_bias=True))\n",
        "    #model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Conv2D(64, (3, 3), strides=2, activation=activ_fct, padding='same', use_bias=True))\n",
        "    #model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Conv2D(64, (3, 3), strides=2, activation=activ_fct, padding='same', use_bias=True))\n",
        "    #model.add(layers.MaxPooling2D((2,2), padding='same'))\n",
        "    model.add(layers.Flatten())\n",
        "    \n",
        "    return model\n",
        "# At this point the representation is a 1568-vector\n",
        "# The convolutions here don't decrease the 2D dimension because they have default (1,1) strides. The MaxPooling does"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfeg_e_j5eGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder0 = make_encoder_model(activ_fct='relu')\n",
        "encoder1 = make_encoder_model(activ_fct='relu')\n",
        "#encoder0.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBhiqjQpRpVu",
        "colab_type": "text"
      },
      "source": [
        "## Generators\n",
        "\n",
        "The generator_start is the biggest part of ExplainGAN, i.e. the one with the most weights. I already ran into one OOM error with this version of the code after building ~5 models with 123 millions of weights. The vast majority of weights appears in the very first, dense, layer. \n",
        "\n",
        "Make this smaller somehow. Let's try to create fewer 7*7 images (in MNIST, we created 128 of these from an encoded vector of length 128). Try 128 for now. Then we have only 10 millions of weights in generator_start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GODGYc5xBfB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This generator is based on the DCGAN generator. But now, we need 3 outputs and not just one!\n",
        "# Therefore tear it apart in the middle. We will use the second part three times to get recon and trafo and mask \n",
        "def make_generator_model_start():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*64, use_bias=False, input_shape=(3136,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Create images of size 7*7 with 64 channels, all connected to the 1568 nodes of the encoded original image\n",
        "    # In the next 2 lines, the tensor is actually reshaped into that channel form\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 64)))\n",
        "    assert model.output_shape == (None, 7, 7, 64) # Note: None is the batch size\n",
        "\n",
        "    # now move 5*5*128(channels) filters over the 7*7*128(channels) images in 1,1 strides. Do this for each of the 64 output channels.\n",
        "    # 64 is the number of different filters we apply. Because of the big number of input channels, each filter is already huge\n",
        "    # The number of parameters here is 5*5*128 (filter weights) *64 (number of filters)\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    return model\n",
        "    \n",
        " \n",
        "# Now the second part. Apply this three times to get reconstruction and transformation and mask\n",
        "# Because of the splitting, I also needed to add the input shape in the first layer of this second part.\n",
        "def make_generator_model_end(activ_fct): \n",
        "    # activ_fct is the activation function for the very last layer (sigmoid for mask, tanh for recon and trafo)\n",
        "    # By taking strides of 2, the size of the image gets doubled in length and width.\n",
        "    # This is the case, because we do a backwards convolution. If we get a 7*7 image by taking (2,2)-strides, we must have started with 14*14\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(7,7,64)))\n",
        "    assert model.output_shape == (None, 14, 14, 32)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(14,14,32)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(28,28,16)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False, input_shape=(56,56,8)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation=activ_fct))\n",
        "    assert model.output_shape == (None, 224, 224, 1)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oo_Sb8JDCsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_start = make_generator_model_start()\n",
        "#generator_start.summary()\n",
        "reconstructor = make_generator_model_end(activ_fct='tanh')\n",
        "transformator = make_generator_model_end(activ_fct='tanh')\n",
        "mask = make_generator_model_end(activ_fct='sigmoid')\n",
        "#reconstructor.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91zlfPOjBX0e",
        "colab_type": "text"
      },
      "source": [
        "## Composite images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a6WCRY3BVr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will need to call this function twice to create a composite image of each class\n",
        "# orig_image and created_image are always from the two different classes. \n",
        "# tf.math.multiply performs an element-wise multiplication\n",
        "def create_composite(orig_image, created_image, mask):\n",
        "  composite = tf.add(tf.math.multiply(1-mask,orig_image), tf.math.multiply((mask),created_image))\n",
        "  return composite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhfehQJaQcom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can input both RGB images (: in the last argument in the example below) and grayscale images\n",
        "# The function returns the same format. Just make sure all inputs have this same format\n",
        "#create_composite(valid_data[0:1,:,:,1],valid_data[4:5,:,:,1], valid_data[2:3,:,:,1]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ohS4SiCCWg8",
        "colab_type": "text"
      },
      "source": [
        "## Discriminators\n",
        "In difference to MNIST, we now want to share the last few layers of the discriminators for both classes, i.e. use the same network discriminator_end after two separate discriminator_start networks. This is what is desribed in the paper. For MNIST, the discriminators were very small already, so it seemed to not make a big difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk-8JjVdEQzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only change in discrimiator_start compared to MNIST: Input shape. Rest stays the same\n",
        "# But: Extra layer added in discriminator_end to get a deeper network than for MNIST\n",
        "# Modification: Put sigmoid at the end (nothing there before) to get values between 0 and 1\n",
        "def make_discriminator_model_start():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', use_bias=True,\n",
        "                                     input_shape=[224, 224, 1]))\n",
        "    # number of params for Conv2D: 64(output channels = number of filters) * 26(filter size 5*5*1 + 1 for a bias node)\n",
        "    # In contrast to the generator model, we use bias nodes here. \n",
        "    # Maybe that would have been useless in the generator because of the batch normalizations\n",
        "    # They normalize the input anyway, so any bias would get cancelled again\n",
        "    # We also use dropout here!\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same', use_bias=True))\n",
        "    # reduced the number of filters from 128 to 64 in reaction to an OOM error when training\n",
        "    # number of params for this Conv2D: 128(output channels) * 1601(filter size 5*5*64 + 1 for a bias node)\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model_end():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same', use_bias=True,\n",
        "                                     input_shape=[56, 56, 32]))\n",
        "  model.add(layers.LeakyReLU())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(1, use_bias=True, activation=tf.nn.sigmoid)) \n",
        "  # sigmoid to get value between 0 and 1 (Goodfellow2016 did this in their DCGAN too, so it should be a good idea here)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1dGBDwUFVDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator0 = make_discriminator_model_start()\n",
        "discriminator1 = make_discriminator_model_start()\n",
        "discriminator_end = make_discriminator_model_end()\n",
        "#discriminator0.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4qrO7LlDQqH",
        "colab_type": "text"
      },
      "source": [
        "## Loss function and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57Fh_e6hz4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run ExplainGAN/lossesPneumonia/loss_reconstruction\n",
        "%run ExplainGAN/lossesPneumonia/loss_gan\n",
        "%run ExplainGAN/lossesPneumonia/loss_classifier\n",
        "%run ExplainGAN/lossesPneumonia/losses_prior"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-qqZskMBpr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc0_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "enc1_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "gen_start_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "recon_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "trafo_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "mask_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "disc0_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "disc1_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "disc_end_optimizer = tf.keras.optimizers.Adam(1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P8cI4NdF6V2",
        "colab_type": "text"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJw8ugiB2Sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changes compared to the DCGAN version: test_input is the first real image(s) instead of a random seed\n",
        "def generate_and_save_images(epoch, test_input, index):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  original = test_input[index:index+1,:,:,:]\n",
        "  prediction = classifier.predict(original)\n",
        "  prediction = np.argmax(prediction, axis=1)\n",
        "  if prediction == 0:\n",
        "    z = encoder0(original[:,:,:,1], training=False)\n",
        "  if prediction == 1:\n",
        "    z = encoder1(original[:,:,:,1], training=False)\n",
        "  gen_from_pred = generator_start(z, training=False)\n",
        "  recon_from_pred = reconstructor(gen_from_pred, training=False)\n",
        "  trafo_from_pred = transformator(gen_from_pred, training=False)\n",
        "  mask_from_pred = mask(gen_from_pred, training=False)\n",
        "  comp_from_pred = create_composite(original, trafo_from_pred, mask_from_pred)\n",
        "  \n",
        "  fig = plt.figure(figsize=(20,20))\n",
        "  plt.subplot(1, 5, 1)\n",
        "  plt.imshow(original[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 2)\n",
        "  plt.imshow(recon_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 3)\n",
        "  plt.imshow(trafo_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 4)\n",
        "  plt.imshow(mask_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 5, 5)\n",
        "  plt.imshow(comp_from_pred[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCOQv8cYF89g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(images, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag):\n",
        "  predicted_classes = classifier.predict(images) # classifier model takes (224,224,3) as input\n",
        "  predicted_classes = np.argmax(predicted_classes, axis=1) # now we have the actual predictions\n",
        "  predicted_classes = tf.reshape(predicted_classes, [-1,1]) # make it compatible with tensors below\n",
        "  predicted_classes = tf.cast(predicted_classes, tf.float32) # change type to float for multiplications\n",
        "  \n",
        "  images_grayscale = images[:,:,:,0:1] # everything works on grayscale images -> Use these from now on.\n",
        "  # Every image produced is grayscale unless its name indicates RGB\n",
        "\n",
        "  with tf.GradientTape() as enc0_tape, tf.GradientTape() as enc1_tape, tf.GradientTape() as gen_start_tape, tf.GradientTape() as recon_tape, tf.GradientTape() as trafo_tape, tf.GradientTape() as mask_tape, tf.GradientTape() as disc0_tape, tf.GradientTape() as disc1_tape, tf.GradientTape() as disc_end_tape:\n",
        "    z_as0 = encoder0(images_grayscale)\n",
        "    z_as1 = encoder1(images_grayscale)\n",
        "    gen_from_pred0 = generator_start(z_as0)\n",
        "    gen_from_pred1 = generator_start(z_as1)\n",
        "    recon_from_pred0 = reconstructor(gen_from_pred0)\n",
        "    recon_from_pred1 = reconstructor(gen_from_pred1)\n",
        "    trafo_from_pred0 = transformator(gen_from_pred0)\n",
        "    trafo_from_pred1 = transformator(gen_from_pred1)\n",
        "    mask_from_pred0 = mask(gen_from_pred0)\n",
        "    mask_from_pred1 = mask(gen_from_pred1) \n",
        "    comp_from_pred0 = create_composite(images_grayscale, trafo_from_pred0, mask_from_pred0)\n",
        "    comp_from_pred1 = create_composite(images_grayscale, trafo_from_pred1, mask_from_pred1)\n",
        "    #print(\"recon0 shape:\", recon_from_pred0.shape)\n",
        "\n",
        "    # In comparison to MNIST, the discriminators got split up in two networks. Consequence: Extra stepts here\n",
        "    #real_output0_step = discriminator0(images_grayscale)\n",
        "    #real_output1_step = discriminator1(images_grayscale)\n",
        "    #recon_output0_step = discriminator0(recon_from_pred0)\n",
        "    #recon_output1_step = discriminator1(recon_from_pred1)\n",
        "    #trafo_output0_step = discriminator1(trafo_from_pred0) # now use the opposite discriminators\n",
        "    #trafo_output1_step = discriminator0(trafo_from_pred1)\n",
        "    #comp_output0_step = discriminator1(comp_from_pred0)\n",
        "    #comp_output1_step = discriminator0(comp_from_pred1)\n",
        "\n",
        "    real_output0 = discriminator_end(discriminator0(images_grayscale))\n",
        "    real_output1 = discriminator_end(discriminator1(images_grayscale))\n",
        "    recon_output0 = discriminator_end(discriminator0(recon_from_pred0))\n",
        "    recon_output1 = discriminator_end(discriminator1(recon_from_pred1))\n",
        "    trafo_output0 = discriminator_end(discriminator1(trafo_from_pred0))\n",
        "    trafo_output1 = discriminator_end(discriminator0(trafo_from_pred1))\n",
        "    comp_output0 = discriminator_end(discriminator1(comp_from_pred0))\n",
        "    comp_output1 = discriminator_end(discriminator0(comp_from_pred1))\n",
        "\n",
        "    # Losses\n",
        "    # GAN loss\n",
        "    loss_gan_pred0 = loss_gan(real_output0, recon_output0, trafo_output0, comp_output0)\n",
        "    loss_gan_pred1 = loss_gan(real_output1, recon_output1, trafo_output1, comp_output1)\n",
        "    loss_g0 = tf.math.multiply(1-predicted_classes, loss_gan_pred0)\n",
        "    loss_g1 = tf.math.multiply(predicted_classes, loss_gan_pred1)\n",
        "    loss_g = (loss_g0 + loss_g1) * weight_g\n",
        "\n",
        "    # Classifier loss - first we need to create predictions for our composite images\n",
        "    # Here we need RGB images again. Transform the grayscale composites to RGB first\n",
        "    comp_from_pred0_RGB = np.stack([comp_from_pred0[:,:,:,0], comp_from_pred0[:,:,:,0], comp_from_pred0[:,:,:,0]], axis=3) \n",
        "    comp_from_pred1_RGB = np.stack([comp_from_pred1[:,:,:,0], comp_from_pred1[:,:,:,0], comp_from_pred1[:,:,:,0]], axis=3)\n",
        "    \n",
        "    pred_comp_from_pred0 = classifier(comp_from_pred0_RGB) # RGB input for classifier\n",
        "    pred_comp_from_pred0 = pred_comp_from_pred0[:,1] # not argmax here, we need the probability of pneumonia\n",
        "    pred_comp_from_pred0 = tf.reshape(pred_comp_from_pred0, [-1,1])\n",
        "    pred_comp_from_pred0 = tf.cast(pred_comp_from_pred0, tf.float32)\n",
        "    pred_comp_from_pred1 = classifier(comp_from_pred1_RGB) # RGB input for classifier\n",
        "    pred_comp_from_pred1 = pred_comp_from_pred1[:,1] # not argmax here, we need the probability\n",
        "    pred_comp_from_pred1 = tf.reshape(pred_comp_from_pred1, [-1,1])\n",
        "    pred_comp_from_pred1 = tf.cast(pred_comp_from_pred1, tf.float32)\n",
        "    # now calculate the loss\n",
        "    loss_class_pred0 = loss_classifier0(pred_comp_from_pred0) \n",
        "    loss_class_pred1 = loss_classifier1(pred_comp_from_pred1)\n",
        "    loss_c0 = tf.math.multiply(1-predicted_classes, loss_class_pred0)\n",
        "    loss_c1 = tf.math.multiply(predicted_classes, loss_class_pred1)\n",
        "    loss_c = (loss_c0 + loss_c1) * weight_c\n",
        "\n",
        "    # Recon loss\n",
        "    loss_recon0 = loss_recon(images_grayscale, recon_from_pred0)\n",
        "    loss_recon1 = loss_recon(images_grayscale, recon_from_pred1)\n",
        "    #print(\"loss_recon0 shape:\", loss_recon0.shape)\n",
        "    loss_r0 = tf.math.multiply(1-predicted_classes, loss_recon0) # set the loss for the wrong recons to 0\n",
        "    loss_r1 = tf.math.multiply(predicted_classes, loss_recon1) # set the loss for the wrong recons to 0\n",
        "    loss_r = (loss_r0 + loss_r1) * weight_r\n",
        "\n",
        "    # The 4 prior losses\n",
        "    #print('mask shape:', mask_from_pred0.shape)\n",
        "    loss_const0 = loss_const(images_grayscale, trafo_from_pred0, mask_from_pred0)\n",
        "    loss_const1 = loss_const(images_grayscale, trafo_from_pred1, mask_from_pred1)\n",
        "    loss_cs0 = tf.math.multiply(1-predicted_classes, loss_const0)\n",
        "    loss_cs1 = tf.math.multiply(predicted_classes, loss_const1)\n",
        "    loss_cs = (loss_cs0 + loss_cs1) * weight_cs\n",
        "    loss_count0 = loss_count(mask_from_pred0, kappa)\n",
        "    loss_count1 = loss_count(mask_from_pred1, kappa)\n",
        "    loss_ct0 = tf.math.multiply(1-predicted_classes, loss_count0)\n",
        "    loss_ct1 = tf.math.multiply(predicted_classes, loss_count1)\n",
        "    loss_ct = (loss_ct0 + loss_ct1) * weight_ct\n",
        "    loss_smooth0 = loss_smoothness(mask_from_pred0)\n",
        "    loss_smooth1 = loss_smoothness(mask_from_pred1)\n",
        "    loss_sm0 = tf.math.multiply(1-predicted_classes, loss_smooth0)\n",
        "    loss_sm1 = tf.math.multiply(predicted_classes, loss_smooth1)\n",
        "    loss_sm = (loss_sm0 + loss_sm1) * weight_sm\n",
        "    loss_entropy0 = loss_entropy(mask_from_pred0)\n",
        "    loss_entropy1 = loss_entropy(mask_from_pred1)\n",
        "    loss_en0 = tf.math.multiply(1-predicted_classes, loss_entropy0)\n",
        "    loss_en1 = tf.math.multiply(predicted_classes, loss_entropy1)\n",
        "    loss_en = (loss_en0 + loss_en1) * weight_en\n",
        "    loss_prior = loss_cs + loss_ct + loss_sm + loss_en\n",
        "    \n",
        "    # Add up losses that are used together\n",
        "    #loss_g = 0\n",
        "    #loss_c = 0\n",
        "    #loss_prior = 0\n",
        "    loss_summed = loss_g + loss_r + loss_c\n",
        "    #print(\"loss_summed shape:\", loss_summed.shape)\n",
        "\n",
        "    gradients_of_enc0 = enc0_tape.gradient(loss_summed, encoder0.trainable_variables)\n",
        "    gradients_of_enc1 = enc1_tape.gradient(loss_summed, encoder1.trainable_variables)\n",
        "    gradients_of_gen_start = gen_start_tape.gradient(loss_summed, generator_start.trainable_variables)\n",
        "    gradients_of_recon = recon_tape.gradient(loss_summed, reconstructor.trainable_variables)\n",
        "    gradients_of_trafo = trafo_tape.gradient(loss_g + loss_c, transformator.trainable_variables)\n",
        "    gradients_of_mask = mask_tape.gradient(loss_prior + loss_g + loss_c, mask.trainable_variables)\n",
        "    gradients_of_disc0 = disc0_tape.gradient(-loss_g, discriminator0.trainable_variables)\n",
        "    gradients_of_disc1 = disc1_tape.gradient(-loss_g, discriminator1.trainable_variables)\n",
        "    gradients_of_disc_end = disc_end_tape.gradient(-loss_g, discriminator_end.trainable_variables)\n",
        "\n",
        "    enc0_optimizer.apply_gradients(zip(gradients_of_enc0, encoder0.trainable_variables))\n",
        "    enc1_optimizer.apply_gradients(zip(gradients_of_enc1, encoder1.trainable_variables))\n",
        "    gen_start_optimizer.apply_gradients(zip(gradients_of_gen_start, generator_start.trainable_variables))\n",
        "    recon_optimizer.apply_gradients(zip(gradients_of_recon, reconstructor.trainable_variables))\n",
        "    trafo_optimizer.apply_gradients(zip(gradients_of_trafo, transformator.trainable_variables))\n",
        "    mask_optimizer.apply_gradients(zip(gradients_of_mask, mask.trainable_variables))\n",
        "    disc0_optimizer.apply_gradients(zip(gradients_of_disc0, discriminator0.trainable_variables))      \n",
        "    disc1_optimizer.apply_gradients(zip(gradients_of_disc1, discriminator1.trainable_variables))\n",
        "    disc_end_optimizer.apply_gradients(zip(gradients_of_disc_end, discriminator_end.trainable_variables))\n",
        "\n",
        "    print('###################################################################################')\n",
        "    print('real_output0: ', tf.reduce_max(real_output0), tf.reduce_min(real_output0), tf.reduce_mean(real_output0))\n",
        "    print('recon_output0: ', tf.reduce_max(recon_output0), tf.reduce_min(recon_output0), tf.reduce_mean(recon_output0))\n",
        "    print('trafo_output0: ', tf.reduce_max(trafo_output0), tf.reduce_min(trafo_output0), tf.reduce_mean(trafo_output0))\n",
        "    print('comp_output0: ', tf.reduce_max(comp_output0), tf.reduce_min(comp_output0), tf.reduce_mean(comp_output0))\n",
        "    print('loss_g: ', tf.reduce_max(loss_g), tf.reduce_min(loss_g), tf.reduce_mean(loss_g))\n",
        "    print('loss_c: ', tf.reduce_max(loss_c), tf.reduce_min(loss_c), tf.reduce_mean(loss_c))\n",
        "    print('loss_r: ', tf.reduce_max(loss_r), tf.reduce_min(loss_r), tf.reduce_mean(loss_r))\n",
        "    print('loss_cs: ', tf.reduce_max(loss_cs), tf.reduce_min(loss_cs), tf.reduce_mean(loss_cs))\n",
        "    print('loss_ct: ', tf.reduce_max(loss_ct), tf.reduce_min(loss_ct), tf.reduce_mean(loss_ct))\n",
        "    print('loss_sm: ', tf.reduce_max(loss_sm), tf.reduce_min(loss_sm), tf.reduce_mean(loss_sm))\n",
        "    print('loss_en: ', tf.reduce_max(loss_en), tf.reduce_min(loss_en), tf.reduce_mean(loss_en))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGnnuzm6M-SU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_g = 0.5\n",
        "weight_c = 1.25\n",
        "weight_r = 0.0002\n",
        "weight_cs = 0.001\n",
        "weight_ct = 10\n",
        "weight_sm = 0.05\n",
        "weight_en = 0.05 \n",
        "kappa = 0.1\n",
        "pretraining_flag = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daFoX43XG10x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking, seems to work just fine. Loss changes from step to step\n",
        "#train_step(test_data) # produces OOM error\n",
        "#train_step(valid_data) # still works\n",
        "train_step(train_data[0:2,:,:,:], weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag) # works"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0nSr4fnjB_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch and shuffle the training (and test - just for experiments) data\n",
        "# We can play around with the batch size a bit\n",
        "BATCH_SIZE = 16\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(len(train_labels)).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).shuffle(len(test_labels)).batch(BATCH_SIZE)\n",
        "EPOCHS = 20\n",
        "# batches have the correct shape for the train_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC00BFixFirx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      # Here we use the batching of the dataset\n",
        "      # call the function train_step (defined in the longest code box)\n",
        "      train_step(image_batch, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag)\n",
        "\n",
        "    # Produce images for the GIF as we go (from DCGAN)\n",
        "    #display.clear_output(wait=True)\n",
        "    generate_and_save_images(epoch + 1, train_data, index=0) # still input train_images here. Would be nice to use dataset instead!!\n",
        "    \n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  #display.clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jshz8xPfkPsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OOM error in the training step when loading old weights before.\n",
        "# Try to avoid training in more than one session\n",
        "encoder0.compile\n",
        "encoder1.compile\n",
        "generator_start.compile\n",
        "reconstructor.compile\n",
        "transformator.compile\n",
        "mask.compile\n",
        "discriminator0.compile\n",
        "discriminator1.compile\n",
        "discriminator_end.compile\n",
        "\n",
        "encoder0.load_weights('/content/gdrive/My Drive/Colab Notebooks/encoder0')\n",
        "encoder1.load_weights('/content/gdrive/My Drive/Colab Notebooks/encoder1')\n",
        "generator_start.load_weights('/content/gdrive/My Drive/Colab Notebooks/generator_start')\n",
        "reconstructor.load_weights('/content/gdrive/My Drive/Colab Notebooks/reconstructor')\n",
        "transformator.load_weights('/content/gdrive/My Drive/Colab Notebooks/transformator')\n",
        "mask.load_weights('/content/gdrive/My Drive/Colab Notebooks/mask')\n",
        "discriminator0.load_weights('/content/gdrive/My Drive/Colab Notebooks/discriminator0')\n",
        "discriminator1.load_weights('/content/gdrive/My Drive/Colab Notebooks/discriminator1')\n",
        "discriminator_end.load_weights('/content/gdrive/My Drive/Colab Notebooks/discriminator_end')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ke6PdPHkAxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(train_dataset, EPOCHS, weight_g, weight_c, weight_r, weight_cs, weight_ct, weight_sm, weight_en, kappa, pretraining_flag)\n",
        "# Batch size 16 and 8 respectively:\n",
        "# OOM error in the first layer of the classifier itself when allocating tensor with shape [8or16,224,224,64]\n",
        "# If the classifier itself (100M parameters) is the problem, my ExplainGAN can maybe grow again\n",
        "# Batch size 6 still works. 8 also worked again one time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4tp4eItFTcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set = test_data\n",
        "generate_and_save_images(1,set,index=20)\n",
        "generate_and_save_images(1,set,index=621)\n",
        "generate_and_save_images(1,set,index=622)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8q3i0yJ6XkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder0.save_weights('/content/gdrive/My Drive/Colab Notebooks/encoder0')\n",
        "encoder1.save_weights('/content/gdrive/My Drive/Colab Notebooks/encoder1')\n",
        "generator_start.save_weights('/content/gdrive/My Drive/Colab Notebooks/generator_start')\n",
        "reconstructor.save_weights('/content/gdrive/My Drive/Colab Notebooks/reconstructor')\n",
        "transformator.save_weights('/content/gdrive/My Drive/Colab Notebooks/transformator')\n",
        "mask.save_weights('/content/gdrive/My Drive/Colab Notebooks/mask')\n",
        "discriminator0.save_weights('/content/gdrive/My Drive/Colab Notebooks/discriminator0')\n",
        "discriminator1.save_weights('/content/gdrive/My Drive/Colab Notebooks/discriminator1')\n",
        "discriminator_end.save_weights('/content/gdrive/My Drive/Colab Notebooks/discriminator_end')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}